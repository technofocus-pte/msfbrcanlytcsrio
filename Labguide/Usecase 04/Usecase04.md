# Use Case 04: Analyze data with Apache Spark

**Introduction**

Apache Spark is an open-source engine for distributed data processing,
and is widely used to explore, process, and analyze huge volumes of data
in data lake storage. Spark is available as a processing option in many
data platform products, including Azure HDInsight, Azure Databricks,
Azure Synapse Analytics, and Microsoft Fabric. One of the benefits of
Spark is support for a wide range of programming languages, including
Java, Scala, Python, and SQL; making Spark a very flexible solution for
data processing workloads including data cleansing and manipulation,
statistical analysis and machine learning, and data analytics and
visualization.

Tables in a Microsoft Fabric lakehouse are based on the open
sourceÂ *Delta Lake*Â format for Apache Spark. Delta Lake adds support for
relational semantics for both batch and streaming data operations, and
enables the creation of a Lakehouse architecture in which Apache Spark
can be used to process and query data in tables that are based on
underlying files in a data lake.

In Microsoft Fabric, Dataflows (Gen2) connect to various data sources
and perform transformations in Power Query Online. They can then be used
in Data Pipelines to ingest data into a lakehouse or other analytical
store, or to define a dataset for a Power BI report.

This lab is designed to introduce the different elements of Dataflows
(Gen2), and not create a complex solution that may exist in an
enterprise.

**Objectives**:

- Create a workspace in Microsoft Fabric with the Fabric trial enabled.

- Establish a lakehouse environment and upload data files for analysis.

- Generate a notebook for interactive data exploration and analysis.

- Load data into a dataframe for further processing and visualization.

- Apply transformations to the data using PySpark.

- Save and partition the transformed data for optimized querying.

- Create a table in the Spark metastore for structured data management

- Save DataFrame as a managed delta table named "salesorders."

- Save DataFrame as an external delta table named "external_salesorder"
  with a specified path.

- Describe and compare properties of managed and external tables.

- Execute SQL queries on tables for analysis and reporting.

- Visualize data using Python libraries such as matplotlib and seaborn.

- Establish a data lakehouse in the Data Engineering experience and
  ingest relevant data for subsequent analysis.

- Define a dataflow for extracting, transforming, and loading data into
  the lakehouse.

- Configure data destinations within Power Query to store the
  transformed data in the lakehouse.

- Incorporate the dataflow into a pipeline to enable scheduled data
  processing and ingestion.

- Remove the workspace and associated elements to conclude the exercise.

# Exercise 1: Create a workspace, lakehouse, notebook and load data into dataframe 

## Task 1: Create a workspace 

Before working with data in Fabric, create a workspace with the Fabric
trial enabled.

1.  Open your browser, navigate to the address bar, and type or paste
    the following URL: +++https://app.fabric.microsoft.com/+++ then
    press the **Enter** button.

> **Note**: If you are directed to Microsoft Fabric Home page, then skip
> steps from \#2 to \#4.
>
> ![](./media/image1.png)

2.  In the **Microsoft Fabric** window, enter your credentials, and
    click on the **Submit** button.
    |   |   |
    |---|---|
    | Username | +++@lab.CloudPortalCredential(User1).Username+++ |
    | Password | +++@lab.CloudPortalCredential(User1).Password+++ |

> ![](./media/image2.png)

3.  Then, In the **Microsoft** window enter the password and click on
    the **Sign in** button**.**

> ![A login screen with a red box and blue text Description
> automatically generated](./media/image3.png)

4.  In **Stay signed in?** window, click on the **Yes** button.

> ![A screenshot of a computer error Description automatically
> generated](./media/image4.png)

5.  Fabric home page, selectÂ **+New workspace**Â tile.

> ![A screenshot of a computer Description automatically
> generated](./media/image5.png)

6.  In the **Create a workspace tab**, enter the following details and
    click on the **Apply** button.

    |  |  |
    |-----|----|
    |Name|	+++dp_Fabric@lab.LabInstance.Id+++ (must be a unique Id)| 
    |Description|	This workspace contains Analyze data with Apache Spark|
    |Advanced|	Under License mode, select Fabric capacity|
    |Default storage format	|Small dataset storage format|

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image6.png)
>
> ![](./media/image7.png)

7.  Wait for the deployment to complete. It takes 2-3 minutes to
    complete. When your new workspace opens, it should be empty.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image8.png)

## Task 2: Create a lakehouse and upload files

Now that you have a workspace, itâ€™s time to switch to theÂ *Data
engineering*Â experience in the portal and create a data lakehouse for
the data files youâ€™re going to analyze.

1.  Create a new Eventhouse by clicking on theÂ **+New item**Â button in
    the navigation bar.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image9.png)

2.  Click on the "**Lakehouse**" tile.

![A screenshot of a computer Description automatically
generated](./media/image10.png)

3.  In theÂ **New lakehouse**Â dialog box,
    enterÂ **+++Fabric_lakehouse+++**Â in theÂ **Name**Â field, click on
    theÂ **Create**Â button and open the new lakehouse.

![A screenshot of a computer Description automatically
generated](./media/image11.png)

4.  After a minute or so, a new empty lakehouse will be created. You
    need to ingest some data into the data lakehouse for analysis.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image12.png)

5.  You will see a notification statingÂ **Successfully created SQL
    endpoint**.

![](./media/image13.png)

6.  In the **Explorer** section, under the **fabric_lakehouse**, hover
    your mouse beside **Files folder**, then click on the horizontal
    ellipses **(â€¦)** menu. Navigate and click on **Upload**, then click
    on the **Upload folder** as shown in the below image.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image14.png)

7.  On theÂ **Upload folder** pane that appears on the right side, select
    the **folder icon** under the **Files/** and then browse to
    **C:\LabFiles** and then select the **orders** folder and click on
    the **Upload** button.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image15.png)

8.  In case, the **Upload 3 files to this site?** dialog box appears,
    then click on **Upload** button.

![](./media/image16.png)

9.  In the Upload folder pane, click on the **Upload** button.

> ![](./media/image17.png)

10. After the files have been uploaded **close** the **Upload folder**
    pane.

![A screenshot of a computer Description automatically
generated](./media/image18.png)

11. ExpandÂ **Files**Â and select theÂ **orders**Â folder and verify that
    the CSV files have been uploaded.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image19.png)

## Task 3: Create a notebook

To work with data in Apache Spark, you can create aÂ *notebook*.
Notebooks provide an interactive environment in which you can write and
run code (in multiple languages), and add notes to document it.

1.  On theÂ **Home**Â page while viewing the contents of
    theÂ **orders**Â folder in your datalake, in theÂ **Open
    notebook**Â menu, selectÂ **New notebook**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image20.png)

2.  After a few seconds, a new notebook containing a singleÂ *cell*Â will
    open. Notebooks are made up of one or more cells that can
    containÂ *code*Â orÂ *markdown*Â (formatted text).

![](./media/image21.png)

3.  Select the first cell (which is currently aÂ *code*Â cell), and then
    in the dynamic tool bar at its top-right, use theÂ **Mâ†“**Â button to
    **convert the cell to aÂ markdownÂ cell**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image22.png)

4.  When the cell changes to a markdown cell, the text it contains is
    rendered.

![A screenshot of a computer Description automatically
generated](./media/image23.png)

5.  Use theÂ **ğŸ–‰**Â (Edit) button to switch the cell to editing mode,
    replace all the text then modify the markdown as follows:

    ```
    # Sales order data exploration
    
    Use the code in this notebook to explore sales order data.
    ```

![](./media/image24.png)

![A screenshot of a computer Description automatically
generated](./media/image25.png)

6.  Click anywhere in the notebook outside of the cell to stop editing
    it and see the rendered markdown.

![A screenshot of a computer Description automatically
generated](./media/image26.png)

## Task 4: Load data into a dataframe

Now youâ€™re ready to run code that loads the data into aÂ *dataframe*.
Dataframes in Spark are similar to Pandas dataframes in Python, and
provide a common structure for working with data in rows and columns.

**Note**: Spark supports multiple coding languages, including Scala,
Java, and others. In this exercise, weâ€™ll useÂ *PySpark*, which is a
Spark-optimized variant of Python. PySpark is one of the most commonly
used languages on Spark and is the default language in Fabric notebooks.

1.  With the notebook visible, expand theÂ **Files**Â list and select
    theÂ **orders**Â folder so that the CSV files are listed next to the
    notebook editor.

> ![A screenshot of a computer Description automatically
> generated](./media/image27.png)

2.  Now, hover your mouse to 2019.csv file. Click on the horizontal
    ellipses **(â€¦)** beside 2019.csv. Navigate and click on **Load
    data**, then selectÂ **Spark**. A new code cell containing the
    following code will be added to the notebook:

    ```
    df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
    # df now is a Spark DataFrame containing CSV data from "Files/orders/2019.csv".
    display(df)
    ```
>
> ![A screenshot of a computer Description automatically
> generated](./media/image28.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image29.png)

**Tip**: You can hide the Lakehouse explorer panes on the left by using
theirÂ **Â«**Â icons. Doing

so will help you focus on the notebook.

3.  Use theÂ **â–· Run cell**Â button on the left of the cell to run it.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image30.png)

**Note**: Since this is the first time youâ€™ve run any Spark code, a
Spark session must be started. This means that the first run in the
session can take a minute or so to complete. Subsequent runs will be
quicker.

4.  When the cell command has completed, review the output below the
    cell, which should look similar to this:

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image31.png)

5.  The output shows the rows and columns of data from the 2019.csv
    file. However, note that the column headers donâ€™t look right. The
    default code used to load the data into a dataframe assumes that the
    CSV file includes the column names in the first row, but in this
    case the CSV file just includes the data with no header information.

6.  Modify the code to set theÂ **header**Â option toÂ **false**. Replace
    all the code in the **cell** with the following code and click on
    **â–· Run cell**Â button and review the output

    ```
    df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
    # df now is a Spark DataFrame containing CSV data from "Files/orders/2019.csv".
    display(df)
    ```

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image32.png)

7.  Now the dataframe correctly includes first row as data values, but
    the column names are auto-generated and not very helpful. To make
    sense of the data, you need to explicitly define the correct schema
    and data type for the data values in the file.

8.  Replace all the code in the **cell** with the following code and
    click on **â–· Run cell**Â button and review the output

    ```
    from pyspark.sql.types import *
    
    orderSchema = StructType([
        StructField("SalesOrderNumber", StringType()),
        StructField("SalesOrderLineNumber", IntegerType()),
        StructField("OrderDate", DateType()),
        StructField("CustomerName", StringType()),
        StructField("Email", StringType()),
        StructField("Item", StringType()),
        StructField("Quantity", IntegerType()),
        StructField("UnitPrice", FloatType()),
        StructField("Tax", FloatType())
        ])
    
    df = spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")
    display(df)
    ```
 
> ![](./media/image33.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image34.png)

9.  Now the dataframe includes the correct column names (in addition to
    theÂ **Index**, which is a built-in column in all dataframes based on
    the ordinal position of each row). The data types of the columns are
    specified using a standard set of types defined in the Spark SQL
    library, which were imported at the beginning of the cell.

10. Confirm that your changes have been applied to the data by viewing
    the dataframe.

11. Use theÂ **+ Code**Â icon below the cell output to add a new code cell
    to the notebook, and enter the following code in it. Click on **â–·
    Run cell**Â button and review the output

    ```
    display(df)
    ```
> ![](./media/image35.png)

12. The dataframe includes only the data from theÂ **2019.csv**Â file.
    Modify the code so that the file path uses a \* wildcard to read the
    sales order data from all of the files in theÂ **orders**Â folder

13. Use theÂ **+ Code**Â icon below the cell output to add a new code cell
    to the notebook, and enter the following code in it.

    ```
    from pyspark.sql.types import *
    
    orderSchema = StructType([
        StructField("SalesOrderNumber", StringType()),
        StructField("SalesOrderLineNumber", IntegerType()),
        StructField("OrderDate", DateType()),
        StructField("CustomerName", StringType()),
        StructField("Email", StringType()),
        StructField("Item", StringType()),
        StructField("Quantity", IntegerType()),
        StructField("UnitPrice", FloatType()),
        StructField("Tax", FloatType())
        ])
    
    df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")
    display(df)
    ```
> ![](./media/image36.png)

14. Run the modified code cell and review the output, which should now
    include sales for 2019, 2020, and 2021.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image37.png)

**Note**: Only a subset of the rows is displayed, so you may not be able
to see examples from all years.

# Exercise 2: Explore data in a dataframe

The dataframe object includes a wide range of functions that you can use
to filter, group, and otherwise manipulate the data it contains.

## Task 1: Filter a dataframe

1.  Use theÂ **+ Code**Â icon below the cell output to add a new code cell
    to the notebook, and enter the following code in it.

    ```
    customers = df['CustomerName', 'Email']
    print(customers.count())
    print(customers.distinct().count())
    display(customers.distinct())
    ```
> ![](./media/image38.png)

2.  **Run** the new code cell, and review the results. Observe the
    following details:

    - When you perform an operation on a dataframe, the result is a new
      dataframe (in this case, a newÂ **customers**Â dataframe is created
      by selecting a specific subset of columns from
      theÂ **df**Â dataframe)

    - Dataframes provide functions such
      asÂ **count**Â andÂ **distinct**Â that can be used to summarize and
      filter the data they contain.

    - TheÂ dataframe\['Field1', 'Field2', ...\]Â syntax is a shorthand way
      of defining a subset of columns. You can also
      useÂ **select**Â method, so the first line of the code above could
      be written asÂ customers = df.select("CustomerName", "Email")

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image39.png)

3.  Modify the code, replace all the code in the **cell** with the
    following code and click on **â–· Run cell**Â button as follows:

    ```
    customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
    print(customers.count())
    print(customers.distinct().count())
    display(customers.distinct())
    ```

4.  **Run** the modified code to view the customers who have purchased
    theÂ ***Road-250 Red, 52*Â product**. Note that you can â€œ**chain**â€
    multiple functions together so that the output of one function
    becomes the input for the next - in this case, the dataframe created
    by theÂ **select**Â method is the source dataframe for
    theÂ **where**Â method that is used to apply filtering criteria.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image40.png)

## Task 2: Aggregate and group data in a dataframe

1.  Click on **+ Code** and copy and paste the below code and then click
    on **Run cell** button.

    ```
    productSales = df.select("Item", "Quantity").groupBy("Item").sum()
    display(productSales)
    ```
>
> ![](./media/image41.png)

2.  Note that the results show the sum of order quantities grouped by
    product. TheÂ **groupBy**Â method groups the rows byÂ *Item*, and the
    subsequentÂ **sum**Â aggregate function is applied to all of the
    remaining numeric columns (in this case,Â *Quantity*)

3.  Click on **+ Code** and copy and paste the below code and then click
    on **Run cell** button.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image42.png)

    ```
    from pyspark.sql.functions import *
    
    yearlySales = df.select(year("OrderDate").alias("Year")).groupBy("Year").count().orderBy("Year")
    display(yearlySales)
    ```
> ![](./media/image43.png)

4.  Note that the results show the number of sales orders per year. Note
    that theÂ **select**Â method includes a SQLÂ **year**Â function to
    extract the year component of theÂ *OrderDate*Â field (which is why
    the code includes anÂ **import**Â statement to import functions from
    the Spark SQL library). It then uses anÂ **alias**Â method is used to
    assign a column name to the extracted year value. The data is then
    grouped by the derivedÂ *Year*Â column and the count of rows in each
    group is calculated before finally theÂ **orderBy**Â method is used to
    sort the resulting dataframe.

# Exercise 3: Use Spark to transform data files

A common task for data engineers is to ingest data in a particular
format or structure, and transform it for further downstream processing
or analysis.

## Task 1: Use dataframe methods and functions to transform data

1.  Click on + Code and copy and paste the below code

    ```
    from pyspark.sql.functions import *
    
    ## Create Year and Month columns
    transformed_df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))
    
    # Create the new FirstName and LastName fields
    transformed_df = transformed_df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))
    
    # Filter and reorder columns
    transformed_df = transformed_df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName", "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"]
    
    # Display the first five orders
    display(transformed_df.limit(5))
    ```

> ![](./media/image44.png)

2.  **Run** the code to create a new dataframe from the original order
    data with the following transformations:

    - AddÂ **Year**Â andÂ **Month**Â columns based on
      theÂ **OrderDate**Â column.

    - AddÂ **FirstName**Â andÂ **LastName**Â columns based on
      theÂ **CustomerName**Â column.

    - Filter and reorder the columns, removing
      theÂ **CustomerName**Â column.

> ![](./media/image45.png)

3.  Review the output and verify that the transformations have been made
    to the data.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image46.png)

You can use the full power of the Spark SQL library to transform the
data by filtering rows, deriving, removing, renaming columns, and
applying any other required data modifications.

**Tip**: See theÂ [*Spark dataframe
documentation*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)Â to
learn more about the methods of the Dataframe object.

## Task 2: Save the transformed data

1.  **Add a new cell** with the following code to save the transformed
    dataframe in Parquet format (Overwriting the data if it already
    exists). **Run** the cell and wait for the message that the data has
    been saved.

    ```
    transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')
    print ("Transformed data saved!")
    ```
>
> **Note**: Commonly,Â *Parquet*Â format is preferred for data files that
> you will use for further analysis or ingestion into an analytical
> store. Parquet is a very efficient format that is supported by most
> large scale data analytics systems. In fact, sometimes your data
> transformation requirement may simply be to convert data from another
> format (such as CSV) to Parquet!
>
> ![](./media/image47.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image48.png)

2.  Then, in theÂ **Lakehouse explorer**Â pane on the left, in
    theÂ **â€¦**Â menu for theÂ **Files**Â node, selectÂ **Refresh**.

> ![A screenshot of a computer Description automatically
> generated](./media/image49.png)

3.  Click on the **transformed_data**Â folder to verify that it contains
    a new folder namedÂ **orders**, which in turn contains one or more
    **Parquet files**.

> ![A screenshot of a computer Description automatically
> generated](./media/image50.png)

4.  Click on **+ Code** following code to load a new dataframe from the
    parquet files in theÂ **transformed_data -\> orders**Â folder:

    ```
    orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
    display(orders_df)
    ```
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image51.png)

5.  **Run** the cell and verify that the results show the order data
    that has been loaded from the parquet files.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image52.png)

## Task 3: Save data in partitioned files

1.  Add a new cell, Click on **+ Code** with the following code; which
    saves the dataframe, partitioning the data
    byÂ **Year**Â andÂ **Month**. **Run** the cell and wait for the message
    that the data has been saved

    ```
    orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
    print ("Transformed data saved!")
    ```
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image53.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image54.png)

2.  Then, in theÂ **Lakehouse explorer**Â pane on the left, in
    theÂ **â€¦**Â menu for theÂ **Files**Â node, selectÂ **Refresh.**

![A screenshot of a computer Description automatically
generated](./media/image55.png)

3.  Expand theÂ **partitioned_orders**Â folder to verify that it contains
    a hierarchy of folders namedÂ **Year=*xxxx***, each containing
    folders namedÂ **Month=*xxxx***. Each month folder contains a parquet
    file with the orders for that month.

![A screenshot of a computer Description automatically
generated](./media/image56.png)

![A screenshot of a computer Description automatically
generated](./media/image57.png)

> Partitioning data files is a common way to optimize performance when
> dealing with large volumes of data. This technique can significant
> improve performance and make it easier to filter data.

4.  Add a new cell, click on **+ Code** with the following code to load
    a new dataframe from theÂ **orders.parquet**Â file:

    ```
    orders_2021_df = spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=*")
    display(orders_2021_df)
    ```

5.  **Run** the cell and verify that the results show the order data for
    sales in 2021. Note that the partitioning columns specified in the
    path (**Year**Â andÂ **Month**) are not included in the dataframe.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image58.png)

# **Exercise 3: Work with tables and SQL**

As youâ€™ve seen, the native methods of the dataframe object enable you to
query and analyze data from a file quite effectively. However, many data
analysts are more comfortable working with tables that they can query
using SQL syntax. Spark provides aÂ *metastore*Â in which you can define
relational tables. The Spark SQL library that provides the dataframe
object also supports the use of SQL statements to query tables in the
metastore. By using these capabilities of Spark, you can combine the
flexibility of a data lake with the structured data schema and SQL-based
queries of a relational data warehouse - hence the term â€œdata
lakehouseâ€.

## Task 1: Create a managed table

Tables in a Spark metastore are relational abstractions over files in
the data lake. tables can beÂ *managed*Â (in which case the files are
managed by the metastore) orÂ *external*Â (in which case the table
references a file location in the data lake that you manage
independently of the metastore).

1.  Add a new code, click on **+ Code** cell to the notebook and enter
    the following code, which saves the dataframe of sales order data as
    a table namedÂ **salesorders**:

    ```
    # Create a new table
    df.write.format("delta").saveAsTable("salesorders")
    
    # Get the table description
    spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
    ```

**Note**: Itâ€™s worth noting a couple of things about this example.
Firstly, no explicit path is provided, so the files for the table will
be managed by the metastore. Secondly, the table is saved
inÂ **delta**Â format. You can create tables based on multiple file
formats (including CSV, Parquet, Avro, and others) butÂ *delta lake*Â is a
Spark technology that adds relational database capabilities to tables;
including support for transactions, row versioning, and other useful
features. Creating tables in delta format is preferred for data
lakehouses in Fabric.

2.  **Run** the code cell and review the output, which describes the
    definition of the new table.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image59.png)

3.  In theÂ **Lakehouse** **explorer**Â pane, in theÂ **â€¦**Â menu for
    theÂ **Tables**Â folder, select **Refresh.**

![A screenshot of a computer Description automatically
generated](./media/image60.png)

4.  Then, expand theÂ **Tables**Â node and verify that
    theÂ **salesorders**Â table has been created under the **dbo** schema.

> ![A screenshot of a computer Description automatically
> generated](./media/image61.png)

5.  Hover your mouse beside **salesorders** table, then click on the
    horizontal ellipses (â€¦). Navigate and click on **Load data**, then
    select **Spark**.

> ![](./media/image62.png)

6.  Click on **â–· Run cell**Â button and which uses the Spark SQL library
    to embed a SQL query against theÂ **salesorder**Â table in PySpark
    code and load the results of the query into a dataframe.

    ```
    df = spark.sql("SELECT * FROM [your_lakehouse].salesorders LIMIT 1000")
    display(df)
    ```

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image63.png)

## Task 2: Create anÂ externalÂ table

You can also createÂ *external*Â tables for which the schema metadata is
defined in the metastore for the lakehouse, but the data files are
stored in an external location.

1.  Under the results returned by the first code cell, use theÂ **+
    Code**Â button to add a new code cell if one doesnâ€™t already exist.
    Then enter the following code in the new cell.

    ```
    df.write.format("delta").saveAsTable("external_salesorder", path="<abfs_path>/external_salesorder")
    ```

![A screenshot of a computer Description automatically
generated](./media/image64.png)

2.  In theÂ **Lakehouse explorer**Â pane, in theÂ **â€¦**Â menu for
    theÂ **Files**Â folder, selectÂ **Copy ABFS path** in the notepad.

> The ABFS path is the fully qualified path to theÂ **Files**Â folder in
> the OneLake storage for your lakehouse - similar to this:

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image65.png)

3.  Now, move into the code cell, replace **\<abfs_path\>** with the
    **path** you copied to the notepad so that the code saves the
    dataframe as an external table with data files in a folder named
    **external_salesorder** in your **Files** folder location. The full
    path should look similar to this

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

4.  Use theÂ **â–·Â (*Run cell*)** button on the left of the cell to run it.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image66.png)

5.  In theÂ **Lakehouse explorer**Â pane, in theÂ **â€¦**Â menu for
    theÂ **Tables**Â folder, select the **Refresh**.

![A screenshot of a computer Description automatically
generated](./media/image67.png)

6.  Then expand theÂ **Tables**Â node and verify that
    theÂ **external_salesorder**Â table has been created.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image68.png)

7.  In theÂ **Lakehouse explorer**Â pane, in theÂ **â€¦**Â menu for
    theÂ **Files**Â folder, selectÂ **Refresh**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image69.png)

8.  Then expand theÂ **Files**Â node and verify that
    theÂ **external_salesorder**Â folder has been created for the tableâ€™s
    data files.

![](./media/image70.png)

## Task 3: CompareÂ managedÂ andÂ externalÂ tables

Letâ€™s explore the differences between managed and external tables.

1.  Under the results returned by the code cell, use theÂ **+
    Code**Â button to add a new code cell. Copy the code below into the
    Code cell and use theÂ **â–·Â (*Run cell*)** button on the left of the
    cell to run it.
    ```
    %%sql
    
    DESCRIBE FORMATTED salesorders;
    ```
>
> ![](./media/image71.png)

2.  In the results, view theÂ **Location**Â property for the table, which
    should be a path to the OneLake storage for the lakehouse ending
    withÂ **/Tables/salesorders**Â (you may need to widen theÂ **Data
    type**Â column to see the full path).

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image72.png)

3.  Modify theÂ **DESCRIBE**Â command to show the details of
    theÂ **external_saleorder**Â table as shown here.

4.  Under the results returned by the code cell, use theÂ **+
    Code**Â button to add a new code cell. Copy the below code and use
    theÂ **â–·Â (*Run cell*)** button on the left of the cell to run it.

    ```
    %%sql
    
    DESCRIBE FORMATTED external_salesorder;
    ```

5.  In the results, view theÂ **Location**Â property for the table, which
    should be a path to the OneLake storage for the lakehouse ending
    withÂ **/Files/external_saleorder**Â (you may need to widen theÂ **Data
    type**Â column to see the full path).

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image73.png)

## Task 4: Run SQL code in a cell

While itâ€™s useful to be able to embed SQL statements into a cell
containing PySpark code, data analysts often just want to work directly
in SQL.

1.  Click on **+ Code** cell to the notebook, and enter the following
    code in it. Click on **â–· Run cell**Â button and review the results.
    Observe that:

    - TheÂ %%sqlÂ line at the beginning of the cell (called aÂ *magic*)
      indicates that the Spark SQL language runtime should be used to
      run the code in this cell instead of PySpark.

    - The SQL code references theÂ **salesorders**Â table that you created
      previously.

    - The output from the SQL query is automatically displayed as the
      result under the cell

      ```
      %%sql
      SELECT YEAR(OrderDate) AS OrderYear,
             SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
      FROM salesorders
      GROUP BY YEAR(OrderDate)
      ORDER BY OrderYear;
      ```

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image74.png)

**Note**: For more information about Spark SQL and dataframes, see
theÂ [*Spark SQL
documentation*](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html).

# Exercise 4: Visualize data with Spark

A picture is proverbially worth a thousand words, and a chart is often
better than a thousand rows of data. While notebooks in Fabric include a
built in chart view for data that is displayed from a dataframe or Spark
SQL query, it is not designed for comprehensive charting. However, you
can use Python graphics libraries likeÂ **matplotlib**Â andÂ **seaborn**Â to
create charts from data in dataframes.

## Task 1: View results as a chart

1.  Click on **+ Code** cell to the notebook, and enter the following
    code in it. Click on **â–· Run cell**Â button and observe that it
    returns the data from theÂ **salesorders**Â view you created
    previously.

    ```
    %%sql
    SELECT * FROM salesorders
    ```

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image75.png)

2.  In the results section beneath the cell, change theÂ **View**Â option
    fromÂ **Table**Â toÂ **+New chart**.

![](./media/image76.png)

3.  Use theÂ **Start editing**Â button at the top right of the chart to
    display the options pane for the chart. Then set the options as
    follows and selectÂ **Apply**:

    - **Chart type**: Bar chart

    - **Key**: Item

    - **Values**: Quantity

    - **Series Group**:Â *leave blank*

    - **Aggregation**: Sum

    - **Stacked**:Â *Unselected*

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image77.png)

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image78.png)

4.  Verify that the chart looks similar to this

> ![](./media/image79.png)

## Task 2: Get started withÂ matplotlib

1.  Click on **+ Code** and copy and paste the below code. **Run** the
    code and observe that it returns a Spark dataframe containing the
    yearly revenue.

    ```
    sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \
                    SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \
                FROM salesorders \
                GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \
                ORDER BY OrderYear"
    df_spark = spark.sql(sqlQuery)
    df_spark.show()
    ```
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image80.png)

2.  To visualize the data as a chart, weâ€™ll start by using
    theÂ **matplotlib**Â Python library. This library is the core plotting
    library on which many others are based, and provides a great deal of
    flexibility in creating charts.

3.  Click on **+ Code** and copy and paste the below code.

    ```
    from matplotlib import pyplot as plt
    
    # matplotlib requires a Pandas dataframe, not a Spark one
    df_sales = df_spark.toPandas()
    
    # Create a bar plot of revenue by year
    plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])
    
    # Display the plot
    plt.show()
    ```

![A screenshot of a computer Description automatically
generated](./media/image81.png)

5.  Click on the **Run cell**Â button and review the results, which
    consist of a column chart with the total gross revenue for each
    year. Note the following features of the code used to produce this
    chart:

    - TheÂ **matplotlib**Â library requires aÂ *Pandas*Â dataframe, so you
      need to convert theÂ *Spark*Â dataframe returned by the Spark SQL
      query to this format.

    - At the core of theÂ **matplotlib**Â library is
      theÂ **pyplot**Â object. This is the foundation for most plotting
      functionality.

    - The default settings result in a usable chart, but thereâ€™s
      considerable scope to customize it

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image82.png)

6.  Modify the code to plot the chart as follows, replace all the code
    in the **cell** with the following code and click on **â–· Run
    cell**Â button and review the output

    ```
    from matplotlib import pyplot as plt
    
    # Clear the plot area
    plt.clf()
    
    # Create a bar plot of revenue by year
    plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')
    
    # Customize the chart
    plt.title('Revenue by Year')
    plt.xlabel('Year')
    plt.ylabel('Revenue')
    plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
    plt.xticks(rotation=45)
    
    # Show the figure
    plt.show()
    ```
 
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image83.png)
>
> ![A graph with orange bars AI-generated content may be
> incorrect.](./media/image84.png)

7.  The chart now includes a little more information. A plot is
    technically contained with aÂ **Figure**. In the previous examples,
    the figure was created implicitly for you; but you can create it
    explicitly.

8.  Modify the code to plot the chart as follows, replace all the code
    in the **cell** with the following code.

    ```
    from matplotlib import pyplot as plt
    
    # Clear the plot area
    plt.clf()
    
    # Create a Figure
    fig = plt.figure(figsize=(8,3))
    
    # Create a bar plot of revenue by year
    plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')
    
    # Customize the chart
    plt.title('Revenue by Year')
    plt.xlabel('Year')
    plt.ylabel('Revenue')
    plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
    plt.xticks(rotation=45)
    
    # Show the figure
    plt.show()
    ```

9.  **Re-run** the code cell and view the results. The figure determines
    the shape and size of the plot.

> A figure can contain multiple subplots, each on its ownÂ *axis*.
>
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image85.png)
>
> ![A screenshot of a graph AI-generated content may be
> incorrect.](./media/image86.png)

10. Modify the code to plot the chart as follows. **Re-run** the code
    cell and view the results. The figure contains the subplots
    specified in the code.

    ```
    from matplotlib import pyplot as plt
    
    # Clear the plot area
    plt.clf()
    
    # Create a figure for 2 subplots (1 row, 2 columns)
    fig, ax = plt.subplots(1, 2, figsize = (10,4))
    
    # Create a bar plot of revenue by year on the first axis
    ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')
    ax[0].set_title('Revenue by Year')
    
    # Create a pie chart of yearly order counts on the second axis
    yearly_counts = df_sales['OrderYear'].value_counts()
    ax[1].pie(yearly_counts)
    ax[1].set_title('Orders per Year')
    ax[1].legend(yearly_counts.keys().tolist())
    
    # Add a title to the Figure
    fig.suptitle('Sales Data')
    
    # Show the figure
    plt.show()
    ```
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image87.png)
>
> ![A screenshot of a computer screen AI-generated content may be
> incorrect.](./media/image88.png)

**Note**: To learn more about plotting with matplotlib, see
theÂ [*matplotlib documentation*](https://matplotlib.org/).

## Task 3: Use theÂ seabornÂ library

WhileÂ **matplotlib**Â enables you to create complex charts of multiple
types, it can require some complex code to achieve the best results. For
this reason, over the years, many new libraries have been built on the
base of matplotlib to abstract its complexity and enhance its
capabilities. One such library isÂ **seaborn**.

1.  Click on **+ Code** and copy and paste the below code.

    ```
    import seaborn as sns
    
    # Clear the plot area
    plt.clf()
    
    # Create a bar chart
    ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
    plt.show()
    ```
2.  **Run** the code and observe that it displays a bar chart using the
    seaborn library.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image89.png)

3.  **Modify** the code as follows. **Run** the modified code and note
    that seaborn enables you to set a consistent color theme for your
    plots.
    ```
    import seaborn as sns
    
    # Clear the plot area
    plt.clf()
    
    # Set the visual theme for seaborn
    sns.set_theme(style="whitegrid")
    
    # Create a bar chart
    ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
    plt.show()
    ```
> ![A screenshot of a graph AI-generated content may be
> incorrect.](./media/image90.png)

4.  **Modify** the code again as follows. **Run** the modified code to
    view the yearly revenue as a line chart.

    ```
    import seaborn as sns
    
    # Clear the plot area
    plt.clf()
    
    # Create a bar chart
    ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)
    plt.show()
    ```
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image91.png)

**Note**: To learn more about plotting with seaborn, see theÂ [*seaborn
documentation*](https://seaborn.pydata.org/index.html).

## Task 4: Use delta tables for streaming data

Delta lake supports streaming data. Delta tables can be aÂ *sink*Â or
aÂ *source*Â for data streams created using the Spark Structured Streaming
API. In this example, youâ€™ll use a delta table as a sink for some
streaming data in a simulated internet of things (IoT) scenario.

1.  Click on **+ Code** and copy and paste the below code and then click
    on **Run cell** button.

    ```
    from notebookutils import mssparkutils
    from pyspark.sql.types import *
    from pyspark.sql.functions import *
    
    # Create a folder
    inputPath = 'Files/data/'
    mssparkutils.fs.mkdirs(inputPath)
    
    # Create a stream that reads data from the folder, using a JSON schema
    jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
    ])
    iotstream = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)
    
    # Write some event data to the folder
    device_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''
    mssparkutils.fs.put(inputPath + "data.txt", device_data, True)
    print("Source stream created...")
    ```
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image92.png)
>
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image93.png)

2.  Ensure the messageÂ ***Source stream createdâ€¦***Â is printed. The code
    you just ran has created a streaming data source based on a folder
    to which some data has been saved, representing readings from
    hypothetical IoT devices.

3.  Click on **+ Code** and copy and paste the below code and then click
    on **Run cell** button.

    ```
    # Write the stream to a delta table
    delta_stream_table_path = 'Tables/iotdevicedata'
    checkpointpath = 'Files/delta/checkpoint'
    deltastream = iotstream.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(delta_stream_table_path)
    print("Streaming to delta sink...")
    ```
> ![](./media/image94.png)

4.  This code writes the streaming device data in delta format to a
    folder namedÂ **iotdevicedata**. Because the path for the folder
    location is in theÂ **Tables**Â folder, a table will automatically be
    created for it. Click on the horizontal ellipses beside table, then
    click on **Refresh**.

![](./media/image95.png)

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image96.png)

5.  Click on **+ Code** and copy and paste the below code and then click
    on **Run cell** button.

    ```
    %%sql
    
    SELECT * FROM IotDeviceData;
    ```
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image97.png)

6.  This code queries theÂ **IotDeviceData**Â table, which contains the
    device data from the streaming source.

7.  Click on **+ Code** and copy and paste the below code and then click
    on **Run cell** button.

    ```
    # Add more data to the source stream
    more_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''
    
    mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)
    ```
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image98.png)

8.  This code writes more hypothetical device data to the streaming
    source.

9.  Click on **+ Code** and copy and paste the below code and then click
    on **Run cell** button.

    ```
    %%sql
    
    SELECT * FROM IotDeviceData;
    ```
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image99.png)

10. This code queries theÂ **IotDeviceData**Â table again, which should
    now include the additional data that was added to the streaming
    source.

11. Click on **+ Code** and copy and paste the below code and then click
    on **Run cell** button.

    ```
    deltastream.stop()
    ```
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image100.png)

12. This code stops the stream.

## Task 5: Save the notebook and end the Spark session

Now that youâ€™ve finished working with the data, you can save the
notebook with a meaningful name and end the Spark session.

1.  In the notebook menu bar, use the âš™ï¸Â **Settings**Â icon to view the
    notebook settings.

![A screenshot of a computer Description automatically
generated](./media/image101.png)

2.  Set theÂ **Name**Â of the notebook toÂ +++**Explore Sales Orders+++**,
    and then close the settings pane.

![A screenshot of a computer Description automatically
generated](./media/image102.png)

3.  On the notebook menu, selectÂ **Stop session**Â to end the Spark
    session.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image103.png)

![A screenshot of a computer Description automatically
generated](./media/image104.png)

# Exercise 5: Create a Dataflow (Gen2) in Microsoft Fabric

In Microsoft Fabric, Dataflows (Gen2) connect to various data sources
and perform transformations in Power Query Online. They can then be used
in Data Pipelines to ingest data into a lakehouse or other analytical
store, or to define a dataset for a Power BI report.

This exercise is designed to introduce the different elements of
Dataflows (Gen2), and not create a complex solution that may exist in an
enterprise

## Task 1: Create a Dataflow (Gen2) to ingest data

Now that you have a lakehouse, you need to ingest some data into it. One
way to do this is to define a dataflow that encapsulates anÂ *extract,
transform, and load*Â (ETL) process.

1.  Now, click on **Fabric_lakehouse** on the left-sided navigation
    pane.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image105.png)

2.  In the **Fabric_lakehouse** home page, click on the drop-down arrow
    in the **Get data** and selectÂ **New Dataflow Gen2.** The Power
    Query editor for your new dataflow opens.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image106.png)

5.  In theÂ **New Dataflow Gen2** dialog box,
    enterÂ **+++Gen2_Dataflow+++**Â in theÂ **Name**Â field, click on
    theÂ **Create**Â button and open the new Dataflow Gen2.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image107.png)

3.  In the **Power Query** pane under the **Home tab**, click on
    **Import from a Text/CSV file**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image108.png)

4.  In the **Connect to data source** pane, under **Connection
    settings**, select **Link to file (Preview)** radio button

- **Link to file**:Â *Selected*

- **File path or
  URL**:Â +++https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv+++

   ![](./media/image109.png)

5.  In the **Connect to data source** pane, under **Connection
    credentials,** enter the following details and click on the **Next**
    button.

- **Connection**: Create new connection

- **Connection name**: Orders

- **data gateway**: (none)

- **Authentication kind**: Anonymous

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image110.png)

6.  In **Preview file data** pane, click on **Create**Â to create the
    data source. ![A screenshot of a computer Description automatically
    generated](./media/image111.png)

7.  The **Power Query** editor shows the data source and an initial set
    of query steps to format the data.

![](./media/image112.png)

8.  On the toolbar ribbon, select theÂ **Add column**Â tab. Then,
    selectÂ **Custom column.**

> ![](./media/image113.png)Â 

9.  Set the New column name to +++**MonthNo+++** , set the Data type to
    **Whole Number** and then add the following
    formula:+++**Date.Month(\[OrderDate\])+++** under **Custom column
    formula**. Select **OK**.

> ![](./media/image114.png)

10. Notice how the step to add the custom column is added to the query.
    The resulting column is displayed in the data pane.

> ![A screenshot of a computer Description automatically
> generated](./media/image115.png)

**Tip:**Â In the Query Settings pane on the right side, notice
theÂ **Applied Steps**Â include each transformation step. At the bottom,
you can also toggle theÂ **Diagram flow**Â button to turn on the Visual
Diagram of the steps.

Steps can be moved up or down, edited by selecting the gear icon, and
you can select each step to see the transformations apply in the preview
pane.

## **Task 2: Add data destination for Dataflow**

1.  On the **Power Query** toolbar ribbon, select theÂ **Home**Â tab. Then
    in theÂ D**ata destination**Â drop-down menu, selectÂ **Lakehouse**(if
    not selected already).

![](./media/image116.png)

![](./media/image117.png)

**Note:**Â If this option is grayed out, you may already have a data
destination set. Check the data destination at the bottom of the Query
settings pane on the right side of the Power Query editor. If a
destination is already set, you can change it using the gear.

2.  TheÂ **Lakehouse**Â destination is indicated as an **icon** in the
    **query** in the Power Query editor.

![A screenshot of a computer Description automatically
generated](./media/image118.png)

![A screenshot of a computer Description automatically
generated](./media/image119.png)

3.  On the Home window, select **Save & run** and click on **Save &
    run** button

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image120.png)

4.  On the left navigation select ***dp_Fabric-XXXXX workspace icon***,
    as shown in the image below

![](./media/image121.png)

## Task 3: Add a dataflow to a pipeline

You can include a dataflow as an activity in a pipeline. Pipelines are
used to orchestrate data ingestion and processing activities, enabling
you to combine dataflows with other kinds of operation in a single,
scheduled process. Pipelines can be created in a few different
experiences, including Data Factory experience.

1.  In the Synapse Data Engineering Home page , Under **dp_FabricXX**
    pane, select **+New item** -\> P**ipeline**

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image122.png)

2.  In theÂ **New pipeline**Â dialog box, enter +++**Load data+++** in
    theÂ **Name**Â field, click on the **Create** button to open the new
    pipeline.

![A screenshot of a computer Description automatically
generated](./media/image123.png)

3.  The pipeline editor opens.

> ![A screenshot of a computer Description automatically
> generated](./media/image124.png)
>
> **Tip**: If the Copy Data wizard opens automatically, close it!

4.  SelectÂ **Pipeline activity**, and add aÂ **Dataflow**Â activity to the
    pipeline.

![A screenshot of a computer Description automatically
generated](./media/image125.png)

5.  With the newÂ **Dataflow1**Â activity selected, on
    theÂ **Settings**Â tab, in theÂ **Dataflow**Â drop-down list,
    selectÂ **Gen2_Dataflow**Â (the data flow you created previously)

![A screenshot of a computer Description automatically
generated](./media/image126.png)

6.  On theÂ **Home**Â tab, save the pipeline using theÂ **ğŸ–«Â (*Save*)**
    icon.

![A screenshot of a computer Description automatically
generated](./media/image127.png)

7.  Use theÂ **â–· Run**Â button to run the pipeline, and wait for it to
    complete. It may take a few minutes.

> ![A screenshot of a computer Description automatically
> generated](./media/image128.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image129.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image130.png)

8.  From the top bar, select **Fabric_lakehouse** tab.

> ![A screenshot of a computer Description automatically
> generated](./media/image131.png)

9.  In **Explorer** pane, select theÂ **â€¦**Â menu forÂ **Tables**,
    selectÂ **refresh**. Then expandÂ **Tables**Â and select
    theÂ **orders**Â table, which has been created by your dataflow.

![A screenshot of a computer Description automatically
generated](./media/image132.png)

![](./media/image133.png)

**Tip**: Use the Power BI DesktopÂ *Dataflows connector*Â to connect
directly to the data transformations done with your dataflow.

You can also make additional transformations, publish as a new dataset,
and distribute with intended audience for specialized datasets.

## Task 4: Clean up resources

In this exercise, youâ€™ve learned how to use Spark to work with data in
Microsoft Fabric.

If youâ€™ve finished exploring your lakehouse, you can delete the
workspace you created for this exercise.

1.  In the bar on the left, select the icon for your workspace to view
    all of the items it contains.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image134.png)

2.  In theÂ **â€¦**Â menu on the toolbar, selectÂ **Workspace settings**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image135.png)

3.  SelectÂ **General** andÂ click on **Remove this workspace.**

![A screenshot of a computer settings Description automatically
generated](./media/image136.png)

4.  In the **Delete workspace?** dialog box, click on the **Delete**
    button.

> ![A screenshot of a computer Description automatically
> generated](./media/image137.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image138.png)

**Summary**

This use case guides you through the process of working with Microsoft
Fabric within Power BI. It covers various tasks, including setting up a
workspace, creating a lakehouse, uploading and managing data files, and
using notebooks for data exploration. Participants will learn how to
manipulate and transform data using PySpark, create visualizations, and
save and partition data for efficient querying.

In this use case, participants will engage in a series of tasks focused
on working with delta tables in Microsoft Fabric. The tasks encompass
uploading and exploring data, creating managed and external delta
tables, comparing their properties, the lab introduces SQL capabilities
for managing structured data and provides insights on data visualization
using Python libraries like matplotlib and seaborn. The exercises aim to
provide a comprehensive understanding of utilizing Microsoft Fabric for
data analysis, and incorporating delta tables for streaming data in an
IoT context.

This use case guides you through the process of setting up a Fabric
workspace, creating a data lakehouse, and ingesting data for analysis.
It demonstrates how to define a dataflow to handle ETL operations and
configure data destinations for storing the transformed data.
Additionally, you'll learn how to integrate the dataflow into a pipeline
for automated processing. Finally, you'll be provided with instructions
to clean up resources once the exercise is complete.

This lab equips you with essential skills for working with Fabric,
enabling you to create and manage workspaces, establish data lakehouses,
and perform data transformations efficiently. By incorporating dataflows
into pipelines, you'll learn how to automate data processing tasks,
streamlining your workflow and enhancing productivity in real-world
scenarios. The cleanup instructions ensure you leave no unnecessary
resources, promoting an organized and efficient workspace management
approach.


