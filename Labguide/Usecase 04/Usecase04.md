# Caso de uso 04: Analisar dados com o Apache Spark

**Introdu√ß√£o**

O Apache Spark √© um motor de c√≥digo aberto para processamento
distribu√≠do de dados, amplamente utilizado para explorar, processar e
analisar grandes volumes de dados em armazenamento de data lake. O Spark
est√° dispon√≠vel como uma op√ß√£o de processamento em muitos produtos de
plataforma de dados, incluindo Azure HDInsight, Azure Databricks, Azure
Synapse Analytics e Microsoft Fabric. Um dos benef√≠cios do Spark √© o
suporte a uma ampla variedade de linguagens de programa√ß√£o, incluindo
Java, Scala, Python e SQL, tornando o Spark uma solu√ß√£o muito flex√≠vel
para cargas de trabalho de processamento de dados, incluindo limpeza e
manipula√ß√£o de dados, an√°lise estat√≠stica e aprendizado de m√°quina, al√©m
de an√°lise e visualiza√ß√£o de dados.

As tabelas em um lakehouse do Microsoft Fabric s√£o baseadas no formato
Delta Lake de c√≥digo aberto para o Apache Spark. O Delta Lake adiciona
suporte √† sem√¢ntica relacional para opera√ß√µes de dados em lote e
streaming e permite a cria√ß√£o de uma arquitetura Lakehouse na qual o
Apache Spark pode ser usado para processar e consultar dados em tabelas
baseadas em arquivos subjacentes em um data lake.

No Microsoft Fabric, os Dataflows (Gen2) conectam-se a v√°rias fontes de
dados e realizam transforma√ß√µes no Power Query Online. Podem ent√£o ser
usados em pipelines de dados para ingest√£o de dados em um lakehouse ou
outro armazenamento anal√≠tico, ou para definir um conjunto de dados para
um relat√≥rio do Power BI.

Este laborat√≥rio foi projetado para apresentar os diferentes elementos
dos Dataflows (Gen2), e n√£o para criar uma solu√ß√£o complexa que possa
existir em uma empresa.

**Objetivos**:

‚Ä¢ Criar um espa√ßo de trabalho no Microsoft Fabric com a vers√£o de
avalia√ß√£o do Fabric ativada.

‚Ä¢ Estabelecer um ambiente lakehouse e carregar arquivos de dados para
an√°lise.

‚Ä¢ Gerar um bloco de notas para explora√ß√£o e an√°lise interativa de dados.

‚Ä¢ Carregar os dados em uma estrutura de dados para processamento e
visualiza√ß√£o adicionais.

‚Ä¢ Aplicar transforma√ß√µes aos dados usando PySpark.

‚Ä¢ Guardar e particionar os dados transformados para otimizar as
consultas.

‚Ä¢ Criar uma tabela no metastore do Spark para gerenciamento estruturado
de dados.

‚Ä¢ Guardar a estrutura de dados como uma tabela delta gerenciada chamada
‚Äúsalesorders‚Äù.

‚Ä¢ Salvar estrutura de dados como uma tabela delta externa chamada
‚Äúexternal_salesorder‚Äù com um caminho especificado.

‚Ä¢ Descrever e comparar as propriedades das tabelas gerenciadas e
externas.

‚Ä¢ Executar consultas SQL nas tabelas para an√°lise e gera√ß√£o de
relat√≥rios.

‚Ä¢ Visualizar os dados usando bibliotecas Python, como matplotlib e
seaborn.

‚Ä¢ Estabelecer um data lakehouse na experi√™ncia de engenharia de dados e
ingest√£o de dados relevantes para an√°lise posterior.

‚Ä¢ Definir um fluxo de dados para extrair, transformar e carregar dados
no lakehouse.

‚Ä¢ Configurar destinos de dados no Power Query para armazenar os dados
transformados no lakehouse.

‚Ä¢ Incorporar o fluxo de dados em um pipeline para permitir o
processamento e a ingest√£o programados de dados.

‚Ä¢ Remover o espa√ßo de trabalho e os elementos associados para concluir o
exerc√≠cio.

# Exerc√≠cio 1: Criar um espa√ßo de trabalho, lakehouse, bloco de notas e carregando dados na estrutura de dados.

## Tarefa 1: Criar um espa√ßo de trabalho

Antes de trabalhar com dados no Fabric, crie um espa√ßo de trabalho com a
vers√£o de avalia√ß√£o do Fabric ativada.

1.  Abra seu navegador, acesse a barra de endere√ßos e digite ou cole a
    seguinte URL: +++https://app.fabric.microsoft.com/+++ Em seguida,
    pressione o bot√£o **Enter**.

> **Observa√ß√£o**: Se voc√™ for direcionado para a p√°gina inicial do
> Microsoft Fabric, ignore as etapas de n¬∫ 2 a n¬∫ 4.
>
> ![](./media/image1.png)

2.  Na janela do **Microsoft Fabric**, insira suas credenciais e clique
    no bot√£o **Submit**.

> ![](./media/image2.png)

3.  Em seguida, na janela da **Microsoft**, digite a senha e clique no
    bot√£o **Sign in.**

> ![A login screen with a red box and blue text Description
> automatically generated](./media/image3.png)

4.  Na janela **Stay signed in?**, clique no bot√£o **Yes.**

> ![A screenshot of a computer error Description automatically
> generated](./media/image4.png)

5.  Na p√°gina inicial do Fabric, selecione o bloco + **New workspace**.

> ![A screenshot of a computer Description automatically
> generated](./media/image5.png)

6.  Na aba **Create a workspace**, insira os seguintes detalhes e clique
    no bot√£o **Apply**.

[TABLE]

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image6.png)
>
> ![](./media/image7.png)

7.  Aguarde a conclus√£o da implementa√ß√£o. Isso leva de 2 a 3 minutos.
    Quando seu novo espa√ßo de trabalho abrir, ele dever√° estar vazio.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image8.png)

## Tarefa 2: Criar um lakehouse e carregar os arquivos

Agora que tem um espa√ßo de trabalho, √© hora de mudar para a experi√™ncia
de engenharia de dados no portal e criar um data lakehouse para os
arquivos de dados que vai analisar.

1.  Crie um novo Eventhouse clicando no bot√£o **+ New item** na barra de
    navega√ß√£o.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image9.png)

2.  Clique no bloco "**Lakehouse**".

![A screenshot of a computer Description automatically
generated](./media/image10.png)

3.  Na caixa de di√°logo **New lakehouse**, digite
    **+++Fabric_lakehouse+++** no campo **Name**, clique no bot√£o
    **Create** e abra o novo lakehouse.

![A screenshot of a computer Description automatically
generated](./media/image11.png)

4.  Ap√≥s cerca de um minuto, um novo lakehouse vazio ser√° criado. Voc√™
    precisa ingerir alguns dados no data lakehouse para an√°lise.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image12.png)

5.  Voc√™ ver√° uma notifica√ß√£o informando **Successfully created SQL
    endpoint**.

![](./media/image13.png)

6.  Na se√ß√£o **Explorer**, abaixo de **fabric_lakehouse**, passe o
    cursor do mouse ao lado da **pasta Files** e clique no menu de
    retic√™ncias horizontais **(...)**. Navegue at√© a pasta e clique em
    **Upload,** depois clique na **Upload folder**, conforme mostrado na
    imagem abaixo.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image14.png)

7.  No painel **Upload folder** que aparece no lado direito, selecione o
    **√≠cone de pasta** em **Files**/ e, em seguida, navegue at√©
    **C:\LabFiles**, selecione a pasta **orders** e clique no bot√£o
    **Upload**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image15.png)

8.  Caso a caixa de di√°logo **Upload 3 files to this site?** seja
    exibida, clique no bot√£o **Upload**.

![](./media/image16.png)

9.  No painel Upload folder, clique no bot√£o **Upload**.

> ![](./media/image17.png)

10. Ap√≥s o carregamento dos arquivos, **feche** o painel **Upload
    folder**.

![A screenshot of a computer Description automatically
generated](./media/image18.png)

11. Expanda **Files**, selecione a pasta **orders** e verifique se os
    arquivos CSV foram carregados.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image19.png)

## Tarefa 3: Criar um bloco de notas

Para trabalhar com dados no Apache Spark, voc√™ pode criar um *bloco de
notas*. Os blocos de notas fornecem um ambiente interativo no qual voc√™
pode escrever e executar c√≥digo (em v√°rias linguagens) e adicionar notas
para document√°-lo.

1.  Na p√°gina **Home**, ao visualizar o conte√∫do da pasta **orders** no
    seu datalake, no menu **Open notebook**, selecione **New notebook**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image20.png)

2.  Ap√≥s alguns segundos, um novo bloco de notas contendo uma √∫nica
    *c√©lula* ser√° aberto. Os blocos de notas s√£o compostos por uma ou
    mais c√©lulas que podem conter ***code***¬†ou ***markdown*** (texto
    formatado).

![](./media/image21.png)

3.  Selecione a primeira c√©lula (que atualmente √© uma c√©lula *de
    c√≥digo*) e, em seguida, na barra de ferramentas din√¢mica no canto
    superior direito, use o bot√£o **M‚Üì** para **converter a c√©lula em
    uma c√©lula Markdown**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image22.png)

4.  Quando a c√©lula se transforma em uma c√©lula Markdown, o texto que
    ela cont√©m √© renderizado.

![A screenshot of a computer Description automatically
generated](./media/image23.png)

5.  Use o bot√£o **üñâ** (Edit) para alternar a c√©lula para o modo de
    edi√ß√£o, substitua todo o texto e, em seguida, modifique o Markdown
    da seguinte forma:

> Copiar c√≥digo
>
> \# Sales order data exploration
>
> Use the code in this notebook to explore sales order data.

![](./media/image24.png)

![A screenshot of a computer Description automatically
generated](./media/image25.png)

6.  Clique em qualquer lugar no bloco de notas fora da c√©lula para parar
    de editar e ver o c√≥digo Markdown renderizado.

![A screenshot of a computer Description automatically
generated](./media/image26.png)

## Tarefa 4: Carregar dados em uma estrutura de dados

Agora est√° pronto para executar o c√≥digo que carrega os dados em uma
estrutura de dados. As estruturas de dados no Spark s√£o semelhantes √†s
estruturas de dados Pandas no Python e fornecem uma estrutura comum para
trabalhar com dados em linhas e colunas.

**Observa√ß√£o**: O Spark suporta v√°rias linguagens de programa√ß√£o,
incluindo Scala, Java e outras. Neste exerc√≠cio, usaremos o PySpark, que
√© uma variante do Python otimizada para o Spark. O PySpark √© uma das
linguagens mais utilizadas no Spark e √© a linguagem padr√£o nos blocos de
notas do Fabric.

1.  Com o bloco de notas vis√≠vel, expanda a lista **Files** e selecione
    a pasta **orders** para que os arquivos CSV sejam listados ao lado
    do editor do bloco de notas.

> ![A screenshot of a computer Description automatically
> generated](./media/image27.png)

2.  Agora, posicione o cursor do mouse sobre o arquivo 2019.csv. Clique
    nas retic√™ncias horizontais **(...)** ao lado de 2019.csv. Navegue
    at√© a op√ß√£o **Load data** e clique nela, depois selecione **Spark**.
    Uma nova c√©lula de c√≥digo contendo o seguinte c√≥digo ser√° adicionada
    ao bloco de notas:

> Copiar c√≥digo
>
> df =
> spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image28.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image29.png)

**Dica**: Voc√™ pode ocultar os pain√©is do Lakehouse Explorer √† esquerda
usando os √≠cones ¬´.

Fazer isso ajudar√° voc√™ a se concentrar no bloco de notas.

3.  Use o bot√£o **‚ñ∑ Run cell,** localizado √† esquerda da c√©lula, para
    execut√°-la.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image30.png)

**Observa√ß√£o**: Como esta √© a primeira vez que voc√™ executa um c√≥digo
Spark, √© necess√°rio iniciar uma sess√£o Spark. Isso significa que a
primeira execu√ß√£o na sess√£o pode levar cerca de um minuto para ser
conclu√≠da. As execu√ß√µes subsequentes ser√£o mais r√°pidas.

4.  Quando o comando da c√©lula for conclu√≠do, verifique a sa√≠da abaixo
    da c√©lula, que dever√° ser semelhante a esta:

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image31.png)

5.  A sa√≠da mostra as linhas e colunas de dados do ficheiro 2019.csv. No
    entanto, note que os cabe√ßalhos das colunas n√£o parecem corretos. O
    c√≥digo padr√£o usado para carregar os dados numa estrutura de dados
    assume que o arquivo CSV inclui os nomes das colunas na primeira
    linha, mas, neste caso, o arquivo CSV inclui apenas os dados, sem
    informa√ß√µes de cabe√ßalho.

6.  Modifique o c√≥digo para definir a op√ß√£o **header** como **false**.
    Substitua todo o c√≥digo na **c√©lula** pelo seguinte c√≥digo e clique
    no bot√£o **‚ñ∑ Run cell** e revise o resultado.

> Copiar c√≥digo
>
> df =
> spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
>
> \# df now is a Spark DataFrame containing CSV data from
> "Files/orders/2019.csv".
>
> display(df)

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image32.png)

7.  Agora, a estrutura de dados inclui corretamente a primeira linha
    como valores de dados, mas os nomes das colunas s√£o gerados
    automaticamente e n√£o s√£o muito √∫teis. Para dar sentido aos dados, √©
    necess√°rio definir explicitamente o esquema e o tipo de dados
    corretos para os valores de dados no arquivo.

8.  Substitua todo o c√≥digo na **c√©lula** pelo seguinte c√≥digo e clique
    no bot√£o **‚ñ∑ Run cell** e revise o resultado.

> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> StructField("SalesOrderNumber", StringType()),
>
> StructField("SalesOrderLineNumber", IntegerType()),
>
> StructField("OrderDate", DateType()),
>
> StructField("CustomerName", StringType()),
>
> StructField("Email", StringType()),
>
> StructField("Item", StringType()),
>
> StructField("Quantity", IntegerType()),
>
> StructField("UnitPrice", FloatType()),
>
> StructField("Tax", FloatType())
>
> \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")
>
> display(df)
>
> ![](./media/image33.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image34.png)

9.  Agora, a estrutura de dados inclui os nomes de coluna corretos (al√©m
    do **√≠ndice**, que √© uma coluna incorporada em todas as estruturas
    de dados com base na posi√ß√£o ordinal de cada linha). Os tipos de
    dados das colunas s√£o especificados usando um conjunto padr√£o de
    tipos definidos na biblioteca Spark SQL, que foram importados no
    in√≠cio da c√©lula.

10. Confirme se as suas altera√ß√µes foram aplicadas aos dados,
    visualizando a estrutura de dados.

11. Use o √≠cone **+ Code**¬†abaixo da sa√≠da da c√©lula para adicionar uma
    nova c√©lula de c√≥digo ao bloco de notas e insira o seguinte c√≥digo
    nela. Clique no bot√£o **‚ñ∑ Run cell** e revise a sa√≠da.

> Copiar c√≥digo
>
> display(df)
>
> ![](./media/image35.png)

12. A estrutura de dados inclui apenas os dados do arquivo **2019.csv**.
    Modifique o c√≥digo para que o caminho do arquivo utilize um
    caractere curinga\* para ler os dados de pedidos de venda de todos
    os arquivos na pasta **orders.**

13. Use o √≠cone **+ Code** abaixo da sa√≠da da c√©lula para adicionar uma
    nova c√©lula de c√≥digo ao bloco de notas e insira o seguinte c√≥digo
    nela.

Copiar c√≥digo

> from pyspark.sql.types import \*
>
> orderSchema = StructType(\[
>
> ¬† ¬† StructField("SalesOrderNumber", StringType()),
>
> ¬† ¬† StructField("SalesOrderLineNumber", IntegerType()),
>
> ¬† ¬† StructField("OrderDate", DateType()),
>
> ¬† ¬† StructField("CustomerName", StringType()),
>
> ¬† ¬† StructField("Email", StringType()),
>
> ¬† ¬† StructField("Item", StringType()),
>
> ¬† ¬† StructField("Quantity", IntegerType()),
>
> ¬† ¬† StructField("UnitPrice", FloatType()),
>
> ¬† ¬† StructField("Tax", FloatType())
>
> ¬† ¬† \])
>
> df =
> spark.read.format("csv").schema(orderSchema).load("Files/orders/\*.csv")
>
> display(df)
>
> ![](./media/image36.png)

14. Execute a c√©lula de c√≥digo modificada e revise a sa√≠da, que agora
    deve incluir as vendas de 2019, 2020 e 2021.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image37.png)

**Observa√ß√£o**: Apenas um subconjunto das linhas √© exibido, portanto,
voc√™ pode n√£o conseguir ver exemplos de todos os anos.

# Exerc√≠cio 2: Explorar dados em uma estrutura de dados

O objeto de estrutura de dados inclui uma ampla gama de fun√ß√µes que voc√™
pode usar para filtrar, agrupar e manipular os dados que ele cont√©m.

## Tarefa 1: Filtrar uma estrutura de dados

1.  Use o √≠cone **+ Code** abaixo da sa√≠da da c√©lula para adicionar uma
    nova c√©lula de c√≥digo ao bloco de notas e insira o seguinte c√≥digo
    nela.

> customers = df\['CustomerName', 'Email'\]
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())
>
> ![](./media/image38.png)

2.  **Execute** a nova c√©lula de c√≥digo e revise os resultados. Observe
    os seguintes detalhes:

    - Quando voc√™ realiza uma opera√ß√£o em uma estrutura de dados, o
      resultado √© uma nova estrutura de dados (neste caso, uma nova
      estrutura de dados de **clientes** √© criada selecionando um
      subconjunto espec√≠fico de colunas da estrutura de dados **df**).

    - As estruturas de dados fornecem fun√ß√µes como **count** e
      **distinct** que podem ser usadas para resumir e filtrar os dados
      que cont√™m.

    - A sintaxe dataframe\['Field1', 'Field2', ...\]¬†√© uma forma
      abreviada de definir um subconjunto de colunas. Voc√™ tamb√©m pode
      usar o m√©todo **select**, ent√£o a primeira linha do c√≥digo acima
      poderia ser escrita como customers = df.select("CustomerName",
      "Email").

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image39.png)

3.  Modifique o c√≥digo, substitua todo o c√≥digo na **c√©lula** pelo
    seguinte c√≥digo e clique no bot√£o **‚ñ∑ Run cell,** conforme mostrado
    abaixo:

> Copiar c√≥digo
>
> customers = df.select("CustomerName",
> "Email").where(df\['Item'\]=='Road-250 Red, 52')
>
> print(customers.count())
>
> print(customers.distinct().count())
>
> display(customers.distinct())

4.  **Execute** o c√≥digo modificado para visualizar os clientes que
    compraram o ***Road-250 Red, 52*¬†product*.*¬†Observe** que voc√™ pode
    "**encadear**" v√°rias fun√ß√µes de forma que a sa√≠da de uma fun√ß√£o se
    torne a entrada para a pr√≥xima - neste caso, estrutura de dados
    criada pelo m√©todo **select** √© a estrutura de dados de origem para
    o m√©todo **where**, que √© usado para aplicar os crit√©rios de
    filtragem.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image40.png)

## Tarefa 2: Agregar e agrupar dados em uma estrutura de dados

1.  Clique em **+ Code**, copie e cole o c√≥digo abaixo e, em seguida,
    clique no bot√£o **Run cell**.

> **Copiar c√≥digo:**
>
> productSales = df.select("Item", "Quantity").groupBy("Item").sum()
>
> display(productSales)
>
> ![](./media/image41.png)

2.  Observe que os resultados mostram a soma das quantidades dos pedidos
    agrupadas por produto. O m√©todo **groupBy** agrupa as linhas por
    *Item*, e a fun√ß√£o de agrega√ß√£o **sum** subsequente √© aplicada a
    todas as colunas num√©ricas restantes (neste caso, *Quantity* ).

3.  Clique em **+ Code**, copie e cole o c√≥digo abaixo e, em seguida,
    clique no bot√£o **Run cell**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image42.png)

> **Copiar c√≥digo**
>
> from pyspark.sql.functions import \*
>
> yearlySales =
> df.select(year("OrderDate").alias("Year")).groupBy("Year").count().orderBy("Year")
>
> display(yearlySales)
>
> ![](./media/image43.png)

4.  Observe que os resultados mostram o n√∫mero de pedidos de venda por
    ano. Note que o m√©todo **select** inclui uma fun√ß√£o SQL **year**
    para extrair o componente de ano do campo **OrderDate** (motivo pelo
    qual o c√≥digo inclui uma instru√ß√£o de **importa√ß√£o** para importar
    fun√ß√µes da biblioteca Spark SQL). Em seguida, o m√©todo **alias** √©
    usado para atribuir um nome de coluna ao valor de ano extra√≠do. Os
    dados s√£o ent√£o agrupados pela coluna derivada **Year**, e a
    contagem de linhas em cada grupo √© calculada. Por fim, o m√©todo
    **orderBy** √© usado para ordenar a estrutura de dados resultante.

# Exerc√≠cio 3: Usar o Spark para transformar arquivos de dados

Uma tarefa comum para engenheiros de dados √© ingerir dados em um formato
ou estrutura espec√≠ficos e transform√°-los para processamento ou an√°lise
posteriores.

## Tarefa 1: Utilizar m√©todos e fun√ß√µes de estrutura de dados para transformar dados

1.  Clique em + Code e copie e cole o c√≥digo abaixo.

**Copiar c√≥digo**

> from pyspark.sql.functions import \*
>
> \## Create Year and Month columns
>
> transformed_df = df.withColumn("Year",
> year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))
>
> \# Create the new FirstName and LastName fields
>
> transformed_df = transformed_df.withColumn("FirstName",
> split(col("CustomerName"), " ").getItem(0)).withColumn("LastName",
> split(col("CustomerName"), " ").getItem(1))
>
> \# Filter and reorder columns
>
> transformed_df = transformed_df\["SalesOrderNumber",
> "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName",
> "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"\]
>
> \# Display the first five orders
>
> display(transformed_df.limit(5))
>
> ![](./media/image44.png)

2.  **Execute** o c√≥digo para criar uma nova estrutura de dados a partir
    dos dados de pedidos originais com as seguintes transforma√ß√µes:

    - Adicionar colunas **Year** e **Month** com base na coluna
      **OrderDate**.

    - Adicionar as colunas **FirstName** e **LastName** com base na
      coluna **CustomerName.**

    - Filtrar e reordenar as colunas, removendo a coluna
      **CustomerName.**

> ![](./media/image45.png)

3.  Analise o resultado e verifique se as transforma√ß√µes foram aplicadas
    aos dados.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image46.png)

Voc√™ pode usar todo o poder da biblioteca Spark SQL para transformar os
dados, filtrando linhas, derivando, removendo, renomeando colunas e
aplicando quaisquer outras modifica√ß√µes de dados necess√°rias.

**Dica**: Consulte a [*Spark dataframe
documentation*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)
para aprender mais sobre os m√©todos do objeto de estrutura de dados.

## Tarefa 2: Salvar os dados transformados

1.  **Adicione uma nova c√©lula** com o seguinte c√≥digo para salvar a
    estrutura de dados transformado no formato Parquet (sobrescrevendo
    os dados, se j√° existirem). **Execute** a c√©lula e aguarde a
    mensagem de que os dados foram salvos.

> Copiar c√≥digo
>
> transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')
>
> print ("Transformed data saved!")
>
> **Observa√ß√£o**: Geralmente, o formato *Parquet* √© preferido para
> arquivos de dados que ser√£o usados para an√°lises posteriores ou para
> serem inseridos em um reposit√≥rio anal√≠tico. O Parquet √© um formato
> muito eficiente e compat√≠vel com a maioria dos sistemas de an√°lise de
> dados em larga escala. Ali√°s, √†s vezes, sua necessidade de
> transforma√ß√£o de dados pode ser simplesmente converter dados de outro
> formato (como CSV) para Parquet!
>
> ![](./media/image47.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image48.png)

2.  Em seguida, no painel **Lakehouse Explorer** √† esquerda, no menu
    **‚Ä¶** do n√≥ **Files**, selecione **Refresh**.

> ![A screenshot of a computer Description automatically
> generated](./media/image49.png)

3.  Clique na pasta **transformed_data** para verificar se cont√©m uma
    nova pasta chamada **orders**, que por sua vez cont√©m um ou mais
    **arquivos Parquet**.

> ![A screenshot of a computer Description automatically
> generated](./media/image50.png)

4.  Clique em **+ Code** a seguir para carregar uma nova estrutura de
    dados a partir dos arquivos parquet na pasta **transformed_data -\>
    orders**:

> **Copiar c√≥digo**
>
> orders_df =
> spark.read.format("parquet").load("Files/transformed_data/orders")
>
> display(orders_df)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image51.png)

5.  **Execute** a c√©lula e verifique se os resultados mostram os dados
    do pedido que foram carregados dos arquivos Parquet.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image52.png)

## Tarefa 3: Salvar dados em arquivos particionados

1.  Adicione uma nova c√©lula, clique em **+ Code** e insira o seguinte
    c√≥digo, que salva estrutura de dados, particionando os dados por
    **Year** e **Month**. **Execute** a c√©lula e aguarde a mensagem de
    que os dados foram salvos.

> Copiar c√≥digo
>
> orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
>
> print ("Transformed data saved!")
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image53.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image54.png)

2.  Em seguida, no painel **Lakehouse Explorer** √† esquerda, no menu
    **‚Ä¶** do n√≥ **Files**, selecione **Refresh.**

![A screenshot of a computer Description automatically
generated](./media/image55.png)

3.  Expanda a pasta **partitioned_orders** para verificar se cont√©m uma
    hierarquia de pastas com o nome **Year=*xxxx***, cada uma contendo
    pastas com o nome **Month=*xxxx***. Cada pasta de m√™s cont√©m um
    arquivo Parquet com os pedidos daquele m√™s.

![A screenshot of a computer Description automatically
generated](./media/image56.png)

![A screenshot of a computer Description automatically
generated](./media/image57.png)

> O particionamento de arquivos de dados √© uma maneira comum de otimizar
> o desempenho ao lidar com grandes volumes de dados. Essa t√©cnica pode
> melhorar significativamente o desempenho e facilitar a filtragem de
> dados.

4.  Adicione uma nova c√©lula, clique em **+ Code** com o seguinte c√≥digo
    para carregar uma nova estrutura de dados do arquivo
    **orders.parquet**:

> Copiar c√≥digo
>
> orders_2021_df =
> spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=\*")
>
> display(orders_2021_df)

5.  **Execute** a c√©lula e verifique se os resultados mostram os dados
    de pedidos de venda de 2021. Observe que as colunas de
    particionamento especificadas no caminho (**Year**¬†e¬†**Month**) n√£o
    est√£o inclu√≠das na estrutura de dados.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image58.png)

**Exerc√≠cio 3: Trabalhar com tabelas e SQL**

Como voc√™ viu, os m√©todos nativos do objeto de estrutura de dados
permitem consultar e analisar dados de um arquivo com bastante efic√°cia.
No entanto, muitos analistas de dados se sentem mais √† vontade
trabalhando com tabelas que podem ser consultadas usando a sintaxe SQL.
O Spark fornece um metastore no qual voc√™ pode definir tabelas
relacionais. A biblioteca Spark SQL que fornece o objeto de estrutura de
dados tamb√©m suporta o uso de instru√ß√µes SQL para consultar tabelas no
metastore. Ao usar esses recursos do Spark, voc√™ pode combinar a
flexibilidade de um data lake com o esquema de dados estruturado e as
consultas baseadas em SQL de um armazenamento de dados relacional - da√≠
o termo ‚Äúdata lakehouse‚Äù.

**Tarefa 1: Criar uma tabela gerenciada**

As tabelas em um metastore do Spark s√£o abstra√ß√µes relacionais sobre
arquivos no data lake. As tabelas podem ser gerenciadas (caso em que os
arquivos s√£o gerenciados pelo metastore) ou externas (caso em que a
tabela faz refer√™ncia a um local de arquivos no data lake que voc√™
gerencia de forma independente do metastore).

1.  Adicionar um novo c√≥digo, clique na c√©lula **+ Code** no bloco de
    notas e insira o seguinte c√≥digo, que salva estrutura de dados de
    dados de pedidos de venda como uma tabela chamada **salesorders**:

> Copiar c√≥digo
>
> \# Create a new table
>
> df.write.format("delta").saveAsTable("salesorders")
>
> \# Get the table description
>
> spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)

**Observa√ß√£o**: Vale a pena notar alguns pontos sobre este exemplo.
Primeiro, nenhum caminho expl√≠cito √© fornecido, portanto, os arquivos da
tabela ser√£o gerenciados pelo metastore. Segundo, a tabela √© salva no
formato **delta**. Voc√™ pode criar tabelas com base em v√°rios formatos
de arquivo (incluindo CSV, Parquet, Avro e outros), mas *o delta lake* √©
uma tecnologia do Spark que adiciona recursos de banco de dados
relacional √†s tabelas, incluindo suporte a transa√ß√µes, versionamento de
linhas e outros recursos √∫teis. A cria√ß√£o de tabelas no formato delta √©
prefer√≠vel para data lakehouses no Fabric.

2.  **Execute** a c√©lula de c√≥digo e revise a sa√≠da, que descreve a
    defini√ß√£o da nova tabela.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image59.png)

3.  No painel do **Lakehouse Explorer**, no menu **‚Ä¶** da pasta
    **Tables**, selecione **Refresh.**

> ![A screenshot of a computer Description automatically
> generated](./media/image60.png)

4.  Em seguida, expanda o n√≥ **Tables** e verifique se a tabela
    **salesorders** foi criada no esquema **dbo**.

> ![A screenshot of a computer Description automatically
> generated](./media/image61.png)

5.  Posicione o cursor do mouse ao lado da tabela **salesorders** e
    clique nas retic√™ncias horizontais (...). Navegue e clique em **Load
    data** e selecione **Spark**.

> ![](./media/image62.png)

6.  Clique no bot√£o **‚ñ∑ Run cell**, que usa a biblioteca Spark SQL para
    incorporar uma consulta SQL na tabela **salesorder** PySpark e
    carregar os resultados da consulta em uma estrutura de dados.

> Copiar c√≥digo
>
> df = spark.sql("SELECT \* FROM Fabric_lakehouse.dbo.salesorders LIMIT
> 1000")
>
> display(df)

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image63.png)

Tarefa 2: Criar uma tabela externa

Voc√™ tamb√©m pode criar tabelas externas nas quais os metadados de
esquema s√£o definidos no metastore do lakehouse, mas os arquivos de
dados s√£o armazenados em um local externo.

1.  Abaixo dos resultados retornados pela primeira c√©lula de c√≥digo, use
    o bot√£o **+ Code** para adicionar uma nova c√©lula de c√≥digo, caso
    ainda n√£o exista. Em seguida, insira o seguinte c√≥digo na nova
    c√©lula.

> Copiar c√≥digo
>
> df.write.format("delta").saveAsTable("external_salesorder",
> path="\<abfs_path\>/external_salesorder")

![A screenshot of a computer Description automatically
generated](./media/image64.png)

2.  No painel do **Lakehouse Explorer**, no menu **‚Ä¶** da pasta
    **Files**, selecione **Copy ABFS path** no bloco de notas.

> O caminho ABFS √© o caminho completo para a pasta **Files** no
> armazenamento OneLake do seu lakehouse - semelhante a este:

abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image65.png)

3.  Agora, na c√©lula de c√≥digo, substitua **\<abfs_path\>** pelo
    **caminho** que voc√™ copiou para o bloco de notas, para que o c√≥digo
    salve estrutura de dados como uma tabela externa com os arquivos de
    dados em uma pasta chamada **external_salesorder** na sua pasta
    **Files**. O caminho completo deve ser semelhante a este:

> abfss://dp_Fabric29@onelake.dfs.fabric.microsoft.com/Fabric_lakehouse.Lakehouse/Files/external_salesorder

4.  Use o bot√£o **‚ñ∑ (*Run cell*)** √† esquerda da c√©lula para execut√°-la.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image66.png)

5.  No painel do **Lakehouse Explorer**, no menu **‚Ä¶** da pasta
    **Tables**, selecione **Refresh**.

> ![A screenshot of a computer Description automatically
> generated](./media/image67.png)

6.  Em seguida, expanda o n√≥ **Tables** e verifique se a tabela
    **external_salesorder** foi criada.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image68.png)

7.  No painel do **Lakehouse Explorer**, no menu **‚Ä¶** da pasta
    **Files**, selecione **Refresh**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image69.png)

8.  Em seguida, expanda o n√≥ **Files** e verifique se a pasta
    **external_salesorder** foi criada para os arquivos de dados da
    tabela.

> ![](./media/image70.png)

**Tarefa 3: Comparar tabelas gerenciadas e externas**

Vamos explorar as diferen√ßas entre tabelas gerenciadas e tabelas
externas.

1.  Abaixo dos resultados retornados pela c√©lula de c√≥digo, use o bot√£o
    **+ Code** para adicionar uma nova c√©lula de c√≥digo. Copie o c√≥digo
    abaixo para a c√©lula de c√≥digo e use o bot√£o **‚ñ∑ (*Run cell*)** √†
    esquerda da c√©lula para execut√°-lo.

> Copiar SQL
>
> %%sql
>
> DESCRIBE FORMATTED salesorders;
>
> ![](./media/image71.png)

2.  Nos resultados, visualize a propriedade **Location** da tabela, que
    deve ser um caminho para o armazenamento OneLake do lakehouse,
    terminando com **/Tables/ salesorders** (talvez seja necess√°rio
    ampliar a coluna **Data type** para visualizar o caminho completo).

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image72.png)

3.  Modifique o comando **DESCRIBE**¬† para exibir os detalhes da tabela
    **external_saleorder**, conforme mostrado aqui.

4.  Abaixo dos resultados retornados pela c√©lula de c√≥digo, use o bot√£o
    **+ Code** para adicionar uma nova c√©lula de c√≥digo. Copie o c√≥digo
    abaixo e use o bot√£o **‚ñ∑ (*Run cell*)** √† esquerda da c√©lula para
    execut√°-lo.

> Copiar SQL
>
> %%sql
>
> DESCRIBE FORMATTED external_salesorder;

5.  Nos resultados, visualize a propriedade **Location** da tabela, que
    deve ser um caminho para o armazenamento OneLake do lakehouse,
    terminando com **/Files/ external_saleorder** (talvez seja
    necess√°rio ampliar a coluna **Data type** para visualizar o caminho
    completo).

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image73.png)

**Tarefa 4: Executar c√≥digo SQL em uma c√©lula**

Embora seja √∫til poder incorporar instru√ß√µes SQL em uma c√©lula que
cont√©m c√≥digo PySpark, os analistas de dados geralmente preferem
trabalhar diretamente em SQL.

1.  Clique em + **Code** no bloco de notas e insira o c√≥digo a seguir.
    Clique no bot√£o ‚ñ∑ **Run cell** e revise os resultados. Observe que:

    - A linha %%sql no in√≠cio da c√©lula (chamada *magic*) indica que o
      ambiente de execu√ß√£o da linguagem Spark SQL deve ser usado para
      executar o c√≥digo nesta c√©lula em vez do PySpark.

    - O c√≥digo SQL faz refer√™ncia √† tabela **salesorders** que voc√™
      criou anteriormente.

    - O resultado da consulta SQL √© exibido automaticamente abaixo da
      c√©lula.

> Copiar SQL
>
> %%sql
>
> SELECT YEAR(OrderDate) AS OrderYear,
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue
>
> FROM salesorders
>
> GROUP BY YEAR(OrderDate)
>
> ORDER BY OrderYear;

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image74.png)

**Observa√ß√£o**: Para obter mais informa√ß√µes sobre Spark SQL e estrutura
de dados, consulte a [*Spark SQL
documentation*](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html).

**Exerc√≠cio 4: Visualizar dados com o Spark**

Uma imagem vale mais que mil palavras, e um gr√°fico √© muitas vezes
melhor do que mil linhas de dados. Embora os blocos de notas no Fabric
incluam uma visualiza√ß√£o de gr√°fico integrada para dados exibidos a
partir de uma estrutura de dados ou consulta Spark SQL, ela n√£o foi
concebida para a cria√ß√£o de gr√°ficos abrangentes. No entanto, pode
utilizar bibliotecas gr√°ficas Python, como **matplotlib** e **seaborn**,
para criar gr√°ficos a partir de dados em estruturas de dados.

**Tarefa 1: Visualizar os resultados em forma de gr√°fico**

1.  Clique em + **Code** no bloco de notas e insira o c√≥digo a seguir.
    Clique no bot√£o ‚ñ∑ **Run cell** e observe que ele retorna os dados da
    exibi√ß√£o **salesorders** que voc√™ criou anteriormente.

> Copiar SQL
>
> %%sql
>
> SELECT \* FROM salesorders
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image75.png)

2.  Na se√ß√£o de resultados abaixo da c√©lula, altere a op√ß√£o **View** de
    **Table** para + **New chart**.

> ![](./media/image76.png)

3.  Use o bot√£o **Start editing** no canto superior direito do gr√°fico
    para exibir o painel de op√ß√µes. Em seguida, defina as op√ß√µes da
    seguinte forma e selecione **Apply**:

    - **Chart type:** Bar chart

    - **Key:** Item

    - **Values:** Quantity

    - **Series Group:** *deixe em branco*

    - **Aggregation**: Sum

    - **Stacked**: *N√£o selecionado*

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image77.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image78.png)

4.  Verifique se o gr√°fico √© semelhante a este.

> ![](./media/image79.png)

**Tarefa 2: Primeiros passos com o matplotlib**

1.  Clique em **+ Code**, copie e cole o c√≥digo abaixo. **Execute** o
    c√≥digo e observe que ele retorna uma estrutura de dados do Spark
    contendo a receita anual.

> Copiar c√≥digo
>
> sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\
>
> SUM((UnitPrice \* Quantity) + Tax) AS GrossRevenue \\
>
> FROM salesorders \\
>
> GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\
>
> ORDER BY OrderYear"
>
> df_spark = spark.sql(sqlQuery)
>
> df_spark.show()
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image80.png)

2.  Para visualizar os dados em um gr√°fico, come√ßaremos usando a
    biblioteca **matplotlib** do Python. Essa biblioteca √© a base para a
    cria√ß√£o de gr√°ficos, na qual muitas outras se baseiam, e oferece
    grande flexibilidade na cria√ß√£o de gr√°ficos.

3.  Clique em **+ Code** e copie e cole o c√≥digo abaixo.

**Copiar c√≥digo**

> from matplotlib import pyplot as plt
>
> \# matplotlib requires a Pandas dataframe, not a Spark one
>
> df_sales = df_spark.toPandas()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\])
>
> \# Display the plot
>
> ![A screenshot of a computer Description automatically
> generated](./media/image81.png)

4.  Clique no bot√£o **Run cell** e revise os resultados, que consistem
    em um gr√°fico de colunas com a receita bruta total para cada ano.
    Observe as seguintes caracter√≠sticas do c√≥digo usado para gerar este
    gr√°fico:

    - A biblioteca **matplotlib** requer uma estrutura de dados do
      **Pandas**, portanto √© necess√°rio converter a estrutura de dados
      do Spark retornado pela consulta Spark SQL para esse formato.

    - No n√∫cleo da biblioteca **matplotlib** est√° o objeto **pyplot**,
      que √© a base para a maior parte das funcionalidades de cria√ß√£o de
      gr√°ficos.

    - As configura√ß√µes padr√£o resultam em um gr√°fico utiliz√°vel, mas h√°
      um amplo potencial de personaliza√ß√£o.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image82.png)

5.  Modifique o c√≥digo para gerar o gr√°fico da seguinte forma: substitua
    todo o c√≥digo na **c√©lula** pelo c√≥digo a seguir, clique no bot√£o
    **Run cell** e revise o resultado.

> Copiar c√≥digo
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()
>
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image83.png)
>
> ![A graph with orange bars AI-generated content may be
> incorrect.](./media/image84.png)

6.  O gr√°fico agora inclui um pouco mais de informa√ß√£o. Um gr√°fico est√°
    tecnicamente contido em uma **Figura**. Nos exemplos anteriores, a
    figura foi criada implicitamente para voc√™; mas voc√™ pode cri√°-la
    explicitamente.

7.  Modifique o c√≥digo para gerar o gr√°fico da seguinte forma,
    substituindo todo o c√≥digo na **C√©lula** pelo c√≥digo a seguir.

> Copiar c√≥digo
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a Figure
>
> fig = plt.figure(figsize=(8,3))
>
> \# Create a bar plot of revenue by year
>
> plt.bar(x=df_sales\['OrderYear'\], height=df_sales\['GrossRevenue'\],
> color='orange')
>
> \# Customize the chart
>
> plt.title('Revenue by Year')
>
> plt.xlabel('Year')
>
> plt.ylabel('Revenue')
>
> plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y',
> alpha=0.7)
>
> plt.xticks(rotation=45)
>
> \# Show the figure
>
> plt.show()

8.  **Execute novamente** a c√©lula de c√≥digo e visualize os resultados.
    A figura determina o formato e o tamanho do gr√°fico.

> Uma figura pode conter v√°rios subgr√°ficos, cada um em seu pr√≥prio
> *eixo*.
>
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image85.png)
>
> ![A screenshot of a graph AI-generated content may be
> incorrect.](./media/image86.png)

9.  Modifique o c√≥digo para gerar o gr√°fico da seguinte forma. **Execute
    novamente** a c√©lula de c√≥digo e visualize os resultados. A figura
    cont√©m os subgr√°ficos especificados no c√≥digo.

> Copiar c√≥digo
>
> from matplotlib import pyplot as plt
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a figure for 2 subplots (1 row, 2 columns)
>
> fig, ax = plt.subplots(1, 2, figsize = (10,4))
>
> \# Create a bar plot of revenue by year on the first axis
>
> ax\[0\].bar(x=df_sales\['OrderYear'\],
> height=df_sales\['GrossRevenue'\], color='orange')
>
> ax\[0\].set_title('Revenue by Year')
>
> \# Create a pie chart of yearly order counts on the second axis
>
> yearly_counts = df_sales\['OrderYear'\].value_counts()
>
> ax\[1\].pie(yearly_counts)
>
> ax\[1\].set_title('Orders per Year')
>
> ax\[1\].legend(yearly_counts.keys().tolist())
>
> \# Add a title to the Figure
>
> fig.suptitle('Sales Data')
>
> \# Show the figure
>
> plt.show()
>
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image87.png)
>
> ![A screenshot of a computer screen AI-generated content may be
> incorrect.](./media/image88.png)

**Observa√ß√£o:** Para saber mais sobre como criar gr√°ficos com
matplotlib, consulte a [*matplotlib
documentation*](https://matplotlib.org/).

Tarefa 3: Utilizar a biblioteca seaborn

Embora o **matplotlib** permita criar gr√°ficos complexos de v√°rios
tipos, pode ser necess√°rio um c√≥digo complexo para obter os melhores
resultados. Por esse motivo, ao longo dos anos, muitas novas bibliotecas
foram criadas com base no matplotlib para abstrair sua complexidade e
aprimorar suas capacidades. Uma dessas bibliotecas √© o **seaborn**.

1.  Clique em **+ Code** e copie e cole o c√≥digo abaixo.

> Copiar c√≥digo
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()

2.  **Execute** o c√≥digo e observe que ele exibe um gr√°fico de barras
    usando a biblioteca seaborn.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image89.png)

3.  **Modifique** o c√≥digo da seguinte forma. **Execute** o c√≥digo
    modificado e observe que o seaborn permite definir um tema de cores
    consistente para seus gr√°ficos.

> Copiar c√≥digo
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Set the visual theme for seaborn
>
> sns.set_theme(style="whitegrid")
>
> \# Create a bar chart
>
> ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()
>
> ![A screenshot of a graph AI-generated content may be
> incorrect.](./media/image90.png)

4.  **Modifique** o c√≥digo novamente da seguinte forma. **Execute** o
    c√≥digo modificado para visualizar a receita anual em um gr√°fico de
    linhas.

> Copiar c√≥digo
>
> import seaborn as sns
>
> \# Clear the plot area
>
> plt.clf()
>
> \# Create a bar chart
>
> ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)
>
> plt.show()
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image91.png)

**Observa√ß√£o:** Para saber mais sobre como criar gr√°ficos com o seaborn,
consulte a [*seaborn
documentation*](https://seaborn.pydata.org/index.html).

**Tarefa 4: Utilizar tabelas delta para dados de streaming**

O Delta Lake suporta dados de streaming. As tabelas Delta podem servir
como *destino* ou *origem* para fluxos de dados criados usando a API
Spark Structured Streaming. Neste exemplo, voc√™ usar√° uma tabela Delta
como destino para alguns dados de streaming em um cen√°rio simulado de
internet of things (IoT).

1.  Clique em **+ Code**, copie e cole o c√≥digo abaixo e, em seguida,
    clique no bot√£o **Run cell**.

> Copiar c√≥digo
>
> from notebookutils import mssparkutils
>
> from pyspark.sql.types import \*
>
> from pyspark.sql.functions import \*
>
> \# Create a folder
>
> inputPath = 'Files/data/'
>
> mssparkutils.fs.mkdirs(inputPath)
>
> \# Create a stream that reads data from the folder, using a JSON
> schema
>
> jsonSchema = StructType(\[
>
> StructField("device", StringType(), False),
>
> StructField("status", StringType(), False)
>
> \])
>
> iotstream =
> spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger",
> 1).json(inputPath)
>
> \# Write some event data to the folder
>
> device_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"ok"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "data.txt", device_data, True)
>
> print("Source stream created...")
>
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image92.png)
>
> ![A screenshot of a computer program AI-generated content may be
> incorrect.](./media/image93.png)

2.  Certifique-se de que a mensagem ***Source stream created ‚Ä¶*** seja
    exibida. O c√≥digo que voc√™ acabou de executar criou uma fonte de
    dados de streaming com base em uma pasta na qual alguns dados foram
    salvos, representando leituras de dispositivos IoT hipot√©ticos.

3.  Clique em **+ Code**, copie e cole o c√≥digo abaixo e, em seguida,
    clique no bot√£o **Run cell**.

> Copiar c√≥digo
>
> \# Write the stream to a delta table
>
> delta_stream_table_path = 'Tables/iotdevicedata'
>
> checkpointpath = 'Files/delta/checkpoint'
>
> deltastream =
> iotstream.writeStream.format("delta").option("checkpointLocation",
> checkpointpath).start(delta_stream_table_path)
>
> print("Streaming to delta sink...")
>
> ![](./media/image94.png)

4.  Este c√≥digo grava os dados do dispositivo de streaming em formato
    Delta em uma pasta chamada **iotdevicedata**. Como o caminho para a
    localiza√ß√£o da pasta est√° na pasta **Tables**, uma tabela ser√°
    criada automaticamente para isso. Clique nas retic√™ncias horizontais
    ao lado de **Table** e, em seguida, clique em **Refresh**.

> ![](./media/image95.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image96.png)

5.  Clique em **+ Code**, copie e cole o c√≥digo abaixo e, em seguida,
    clique no bot√£o **Run cell**.

> Copiar SQL
>
> %%sql
>
> SELECT \* FROM IotDeviceData;
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image97.png)

6.  Este c√≥digo consulta a tabela **IotDeviceData**, que cont√©m os dados
    do dispositivo provenientes da fonte de streaming.

7.  Clique em **+ Code**, copie e cole o c√≥digo abaixo e, em seguida,
    clique no bot√£o **Run cell**

> Copiar c√≥digo
>
> \# Add more data to the source stream
>
> more_data = '''{"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"ok"}
>
> {"device":"Dev1","status":"error"}
>
> {"device":"Dev2","status":"error"}
>
> {"device":"Dev1","status":"ok"}'''
>
> mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image98.png)

8.  Este c√≥digo grava mais dados hipot√©ticos de dispositivos na origem
    de streaming.

9.  Clique em **+ Code**, copie e cole o c√≥digo abaixo e, em seguida,
    clique no bot√£o **Run cell**.

> Copiar SQL
>
> %%sql
>
> SELECT \* FROM IotDeviceData;
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image99.png)

10. Este c√≥digo consulta novamente a tabela **IotDeviceData**, que agora
    deve incluir os dados adicionais que foram adicionados √† fonte de
    streaming.

11. Clique em **+ Code**, copie e cole o c√≥digo abaixo e, em seguida,
    clique no bot√£o **Run cell**.

> Copiar c√≥digo
>
> deltastream.stop()
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image100.png)

12. Este c√≥digo interrompe o fluxo.

**Tarefa 5: Salvar o bloco de notas e encerrar a sess√£o do Spark**

Agora que voc√™ terminou de trabalhar com os dados, pode salvar o bloco
de notas com um nome significativo e encerrar a sess√£o do Spark.

1.  Na barra de menus do bloco de notas, use o √≠cone de configura√ß√£o
    ‚öôÔ∏è¬†**Settings** para visualizar as configura√ß√µes do bloco de notas.

> ![A screenshot of a computer Description automatically
> generated](./media/image101.png)

2.  Defina o **Name** do bloco de notas como +++**Explore Sales
    Orders+++** e, em seguida, feche o painel de configura√ß√µes.

> ![A screenshot of a computer Description automatically
> generated](./media/image102.png)

3.  No menu do bloco de notas, selecione **Stop session** para encerrar
    a sess√£o do Spark.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image103.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image104.png)

**Exerc√≠cio 5: Criar um fluxo de dados (Gen2) no Microsoft Fabric**

No Microsoft Fabric, os Dataflows (Gen2) conectam-se a v√°rias fontes de
dados e realizam transforma√ß√µes no Power Query Online. Podem ent√£o ser
usados em Pipelines de dados para ingerir dados em um lakehouse ou outro
reposit√≥rio anal√≠tico, ou para definir um conjunto de dados para um
relat√≥rio do Power BI.

Este exerc√≠cio foi concebido para apresentar os diferentes elementos dos
Dataflows (Gen2), e n√£o para criar uma solu√ß√£o complexa que possa
existir em uma empresa.

**Tarefa 1: Criar um fluxo de dados (Gen2) para ingerir dados**

Agora que voc√™ tem um lakehouse, precisa inserir alguns dados nela. Uma
maneira de fazer isso √© definir um fluxo de dados que englobe um
processo de *extract, transform, e load* (ETL).

1.  Agora, clique em **Fabric_lakehouse** no painel de navega√ß√£o √†
    esquerda.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image105.png)

2.  Na p√°gina inicial do **Fabric_lakehouse**, clique na seta suspensa
    em **Get data** e selecione **New Dataflow Gen2**. O editor do Power
    Query para o novo fluxo de dados ser√° aberto.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image106.png)

3.  Na caixa de di√°logo **New Dataflow Gen2**, digite
    **+++Gen2_Dataflow+++** no campo **Name**, clique no bot√£o
    **Create** e abra o **New Dataflow Gen2**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image107.png)

4.  No painel do **Power Query**, na aba **Home**, clique em **Import
    from a Text/CSV file**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image108.png)

5.  No painel **Connect to data source**, em **Connection settings**,
    selecione o bot√£o de op√ß√£o **Link to file (Pr√©-visualiza√ß√£o).**

- **Link to file**: *Selecionado*

- **File path or URL**:
  +++https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv+++

![](./media/image109.png)

6.  No painel **Connect to data source**, em **Connection credentials,**
    insira os seguintes detalhes e clique no bot√£o **Next.**

- **Connection:** Create new connection

- **Connection name:** Orders

- **data gateway:** (none)

- **Authentication kind:** Anonymous

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image110.png)

7.  No painel **Preview file data**, clique em **Create** para criar a
    fonte de dados.![A screenshot of a computer Description
    automatically generated](./media/image111.png)

8.  O editor do **Power Query** exibe a fonte de dados e um conjunto
    inicial de etapas de consulta para formatar os dados.

> ![](./media/image112.png)

9.  Na faixa de op√ß√µes da barra de ferramentas, selecione a aba **Add
    column**. Em seguida, selecione **Custom column.**

> ![](./media/image113.png)¬†

10. Defina o nome da nova coluna como +++**MonthNo+++**, defina o tipo
    de dados como **Whole Number** e adicione a seguinte f√≥rmula:
    +++**Date.Month(\[OrderDate\])+++** em **Custom column formula**.
    Selecione **OK**.

> ![](./media/image114.png)

11. Observe como a etapa para adicionar a coluna personalizada √©
    adicionada √† consulta. A coluna resultante √© exibida no painel de
    dados.

> ![A screenshot of a computer Description automatically
> generated](./media/image115.png)

**Dica:** No painel configura√ß√µes da consulta, √† direita, observe que as
**Applied Steps** incluem cada etapa de transforma√ß√£o. Na parte
inferior, voc√™ tamb√©m pode ativar o bot√£o **Diagram flow** para exibir o
diagrama visual das etapas.

Os degraus podem ser movidos para cima ou para baixo, editados
selecionando o √≠cone de engrenagem, e pode selecionar cada etapa para
ver as transforma√ß√µes aplicadas no painel de pr√©-visualiza√ß√£o.

Tarefa 2: Adicionar destino de dados para o fluxo de dados

1.  Na faixa de op√ß√µes da barra de ferramentas do **Power Query**,
    selecione a aba **Home**. Em seguida, no menu suspenso **Data
    destination,** selecione **Lakehouse** (se ainda n√£o estiver
    selecionado).

> ![](./media/image116.png)
>
> ![](./media/image117.png)

**Observa√ß√£o:** Se esta op√ß√£o estiver acinzentada, voc√™ pode j√° ter um
destino de dados definido. Verifique o destino de dados na parte
inferior do painel **Query settings**, no lado direito do editor do
Power Query. Se um destino j√° estiver definido, voc√™ poder√° alter√°-lo
usando o √≠cone de engrenagem.

2.  O destino **Lakehouse** √© indicado por um **√≠cone** na **query** no
    editor do Power Query.

> ![A screenshot of a computer Description automatically
> generated](./media/image118.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image119.png)

3.  Na janela inicial, selecione **Save & run** e clique no bot√£o **Save
    & run.**

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image120.png)

4.  Na navega√ß√£o √† esquerda, selecione o **√≠cone do espa√ßo de trabalho**
    ***dp_Fabric-XXXXX***, conforme mostrado na imagem abaixo.

> ![](./media/image121.png)

**Tarefa 3: Adicionar um fluxo de dados a um pipeline**

Voc√™ pode incluir um fluxo de dados como uma atividade em um pipeline.
Os pipelines s√£o usados para orquestrar atividades de ingest√£o e
processamento de dados, permitindo combinar fluxos de dados com outros
tipos de opera√ß√£o em um √∫nico processo agendado. Os pipelines podem ser
criados em diferentes experi√™ncias, incluindo a experi√™ncia Data
Factory.

1.  Na p√°gina inicial de engenharia de dados da Synapse, no painel
    **dp_FabricXX**, selecione +**New item** ‚Üí P**ipeline**.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image122.png)

2.  Na caixa de di√°logo **New pipeline**, digite +++**Load data+++** no
    campo **Name** e clique no bot√£o **Create** para abrir o novo
    pipeline.

> ![A screenshot of a computer Description automatically
> generated](./media/image123.png)

3.  O editor de pipeline √© aberto.

> ![A screenshot of a computer Description automatically
> generated](./media/image124.png)
>
> **Dica:** Se o assistente de c√≥pia de dados abrir automaticamente,
> feche-o!

4.  Selecione **Pipeline activity** e adicione uma atividade
    **Dataflow** ao pipeline.

> ![A screenshot of a computer Description automatically
> generated](./media/image125.png)

5.  Com a nova atividade **Dataflow1** selecionada, na aba **Settings**,
    na lista suspensa **Dataflow**, selecione **Gen2_Dataflow** (o fluxo
    de dados que voc√™ criou anteriormente).

> ![A screenshot of a computer Description automatically
> generated](./media/image126.png)

6.  Na aba **Home**, salve o pipeline usando o √≠cone **üñ´ (*Salvar*)**.

> ![A screenshot of a computer Description automatically
> generated](./media/image127.png)

7.  Use o bot√£o **‚ñ∑ Run** para executar o pipeline e aguarde a
    conclus√£o. Isso pode levar alguns minutos.

> ![A screenshot of a computer Description automatically
> generated](./media/image128.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image129.png)
>
> ![A screenshot of a computer Description automatically
> generated](./media/image130.png)

8.  Na barra superior, selecione a aba **Fabric_lakehouse**.

> ![A screenshot of a computer Description automatically
> generated](./media/image131.png)

9.  No painel **Explorer**, selecione o menu **‚Ä¶** para **Tables** e
    selecione **refresh**. Em seguida, expanda **Tables** e selecione a
    tabela **orders**, que foi criada pelo seu fluxo de dados.

> ![A screenshot of a computer Description automatically
> generated](./media/image132.png)
>
> ![](./media/image133.png)

**Dica:** Use o *conector de fluxos de dados do Power BI Desktop* para
se conectar diretamente √†s transforma√ß√µes de dados realizadas com seu
fluxo de dados.

Voc√™ tamb√©m pode realizar transforma√ß√µes adicionais, publicar como um
novo conjunto de dados e distribuir para o p√∫blico-alvo espec√≠fico de
conjuntos de dados especializados.

**Tarefa 4: Limpar recursos**

Neste exerc√≠cio, voc√™ aprendeu como usar o Spark para trabalhar com
dados no Microsoft Fabric.

Se voc√™ j√° terminou de explorar seu lakehouse, pode excluir o espa√ßo de
trabalho que criou para este exerc√≠cio.

1.  Na barra √† esquerda, selecione o √≠cone do seu espa√ßo de trabalho
    para visualizar todos os itens que ele cont√©m.

> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image134.png)

2.  No menu **‚Ä¶** da barra de ferramentas, selecione **Workspace
    settings**.

![A screenshot of a computer AI-generated content may be
incorrect.](./media/image135.png)

3.  Selecione **General** e clique em **Remove this workspace.**

![A screenshot of a computer settings Description automatically
generated](./media/image136.png)

4.  Na caixa de di√°logo **Delete workspace?**, clique no bot√£o
    **Delete.**

> ![A screenshot of a computer Description automatically
> generated](./media/image137.png)
>
> ![A screenshot of a computer AI-generated content may be
> incorrect.](./media/image138.png)

**Resumo**

Este caso de uso orienta voc√™ no processo de trabalho com o Microsoft
Fabric no Power BI. Abrange v√°rias tarefas, incluindo a configura√ß√£o de
um espa√ßo de trabalho, a cria√ß√£o de um lakehouse, o carregamento e
gerenciamento de arquivos de dados e o uso de bloco de notas para
explora√ß√£o de dados. Os participantes aprender√£o como manipular e
transformar dados usando PySpark, criar visualiza√ß√µes e salvar e
particionar dados para consultas eficientes.

Neste caso de uso, os participantes realizar√£o uma s√©rie de tarefas
focadas no trabalho com tabelas delta no Microsoft Fabric. As tarefas
incluem o carregamento e a explora√ß√£o de dados, a cria√ß√£o de tabelas
delta gerenciadas e externas, a compara√ß√£o de suas propriedades, a
introdu√ß√£o de recursos SQL para o gerenciamento de dados estruturados e
insights sobre visualiza√ß√£o de dados usando bibliotecas Python como
matplotlib e seaborn. Os exerc√≠cios visam proporcionar uma compreens√£o
abrangente da utiliza√ß√£o do Microsoft Fabric para an√°lise de dados e da
incorpora√ß√£o de tabelas delta para streaming de dados em um contexto de
IoT.

Este caso de uso orienta voc√™ no processo de configura√ß√£o de um espa√ßo
de trabalho do Fabric, cria√ß√£o de um data lake e ingest√£o de dados para
an√°lise. Ele demonstra como definir um fluxo de dados para lidar com
opera√ß√µes de ETL e configurar destinos de dados para armazenar os dados
transformados. Al√©m disso, voc√™ aprender√° como integrar o fluxo de dados
a um pipeline para processamento automatizado. Por fim, voc√™ receber√°
instru√ß√µes para limpar os recursos ap√≥s a conclus√£o do exerc√≠cio.

Este laborat√≥rio fornece as habilidades essenciais para trabalhar com o
Fabric, permitindo que voc√™ crie e gerencie espa√ßos de trabalho,
estabele√ßa data lakes e execute transforma√ß√µes de dados com efici√™ncia.
Ao incorporar fluxos de dados em pipelines, voc√™ aprender√° a automatizar
tarefas de processamento de dados, otimizando seu fluxo de trabalho e
aumentando a produtividade em cen√°rios reais. As instru√ß√µes de limpeza
garantem que voc√™ n√£o deixe recursos desnecess√°rios, promovendo uma
abordagem organizada e eficiente para o gerenciamento do espa√ßo de
trabalho.
